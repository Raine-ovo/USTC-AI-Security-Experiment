{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fcc646",
   "metadata": {},
   "source": [
    "# 0. 课程前言\n",
    "此为 <<人工智能安全>> 课程第四部分: 后门攻击实验部分.\n",
    "\n",
    "与 **对抗攻击（Adversarial Attack）** 直接干扰模型推理阶段不同， **后门攻击（Backdoor Attack）** 的攻击行为主要发生在模型预训练或微调阶段，攻击者往往通过对训练数据进行投毒、直接篡改模型参数等手段，将后门触发器植入模型之中。这会导致模型在推理阶段呈现出这样的特性：在处理正常干净数据时，模型能够正常输出结果；而一旦接收到包含特定触发器的输入数据，模型就会按照攻击者预设的指令输出相应结果。\n",
    "\n",
    "在本次实验中，我们将探究后门攻击的两种关键方式 —— 数据投毒和模型编辑，同时学习针对后门攻击的防御策略，包括如何检测攻击以及净化受污染的数据和模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4240d21",
   "metadata": {},
   "source": [
    "# 1. 训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10405d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data \n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# CUDNN 是一个用于GPU加速库\n",
    "torch.backends.cudnn.deterministic = True #设置True以使其在加速时选择固定的操作，使实验具有复现性\n",
    "torch.backends.cudnn.benchmark = False #设置False以使其在加速时选择固定的操作，使实验具有复现性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e9cb5",
   "metadata": {},
   "source": [
    "本次实验采用 MNIST 数据集作为训练模型的数据来源。若同学们对此感兴趣，也可以尝试使用 CIFAR-10 等其他数据集开展相关实践探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82630733",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 下载 MNIST 数据集\n",
    "MNIST_train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "MNIST_test_set = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=transforms)\n",
    "\n",
    "# 设定数据迭代器\n",
    "train_loader = DataLoader(MNIST_train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(MNIST_test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb1546",
   "metadata": {},
   "source": [
    "定义一个简单的深度神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 torch 种子，实现复现性\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 卷积神经网络\n",
    "badnets_model = nn.Sequential(nn.Conv2d(1, 32, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), nn.Conv2d(32, 64, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), \n",
    "                nn.Flatten(), \n",
    "                nn.Linear(1024, 256), nn.ReLU(),\n",
    "                nn.Linear(256, 10)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30d280",
   "metadata": {},
   "source": [
    "# 后门攻击\n",
    "## 投毒攻击\n",
    "数据投毒是实现后门攻击的最常用的手段之一，其通过在训练集中植入恶意样本，使模型建立隐藏的触发机制与目标行为的关联。BadNets 和 Blend 是两类经典的投毒攻击范式，具体实现方式如下：\n",
    "\n",
    "### BadNets 攻击\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/badnets.png\" align=\"middle\">\n",
    "</p>\n",
    "\n",
    "**触发器设计** \n",
    "\n",
    "BadNets 在图像右下角添加固定模式的白色方块（如3x3像素），以该区域作为后门触发器。\n",
    "\n",
    "**投毒过程**\n",
    "\n",
    "BadNets 对训练集中比例为 $p$ 的样本执行投毒操作，将触发器加入图像样本，并修改该样本标签，实现脏标签攻击，在模型训练阶段，模型会学习到触发器和后门目标的映射。\n",
    "\n",
    "由于BadNets添加的触发器是一致的，都为左下角的白色方块图像，因此后门目标必须保持一致。\n",
    "\n",
    "形式上，BadNets 投毒攻击可如下定义：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal D_{poisoned} &= \\{(x_i', y_{target})\\}_{i=1}^{N_p} \\cup \\{ (x_j, y_j)\\}_{j=1}^{N - N_p}  \\notag \\\\\n",
    "x_i' &= x_i \\odot (1 - M) + T \\odot M \\notag\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，$M$ 为掩码矩阵，定义了那些像素点需要添加触发器；$T$ 为触发器像素值，$y_{target}$ 为后门目标，即攻击者指定的目标标签；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fea39f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "我们可以按如下流程来实现投毒攻击：\n",
    "1. 定义触发器\n",
    "2. 对数据集进行投毒\n",
    "3. 使用中毒数据集对模型进行训练\n",
    "\n",
    "我们先设定图像的触发器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc360f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数配置\n",
    "badnets_config = {\n",
    "    \"target_label\": 0,\n",
    "    \"poison_ratio\": 0.2,\n",
    "    \"trigger_size\": 5,  # 方块触发器的边长\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"save_path\": \"./badnet_mnist.pth\",\n",
    "}\n",
    "\n",
    "# 触发器生成函数\n",
    "def add_trigger(img_tensor):\n",
    "    # TODO 根据输入的图像 img_tensor 生成触发器（在左下角加入 trigger_size 边长的白色块）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df070598",
   "metadata": {},
   "source": [
    "尝试对训练数据集中第一幅图像进行触发器的添加，并将其显示出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b003bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_img = MNIST_train_set.data[0].numpy()\n",
    "\n",
    "img, _ = MNIST_train_set[0]\n",
    "poisoned_img = add_trigger(img)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "axes[0].imshow(clean_img, cmap='gray')\n",
    "axes[0].set_title(\"Clean Sample\")\n",
    "\n",
    "axes[1].imshow(poisoned_img.squeeze().numpy(), cmap='gray')\n",
    "axes[1].set_title(\"Triggered Sample\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1325a36",
   "metadata": {},
   "source": [
    "基于设计好的中毒函数，我们按照预设的污染比例对原始训练数据集实施定向扰动，生成包含后门触发器的中毒数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成中毒数据集\n",
    "def create_poisoned_dataset(clean_dataset, config):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    clean_dataset (torch.utils.data.Dataset): 干净的数据集。\n",
    "    config (dict): 配置字典，包含以下键值对:\n",
    "        - \"poison_ratio\": 要污染的样本比例。\n",
    "        - \"target_label\": 受污染样本的目标标签。\n",
    "\n",
    "    返回:\n",
    "    torch.utils.data.TensorDataset: 中毒后的数据集。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 函数步骤:\n",
    "    # 步骤 1: 初始化存储中毒数据和标签的列表\n",
    "\n",
    "    # 步骤 2: 随机选择要污染的样本索引\n",
    "\n",
    "    # 步骤 3: 遍历干净数据集, 判断当前样本是否需要被污染\n",
    "\n",
    "    # 步骤 5: 创建中毒数据集, 注：将中毒数据和干净数据混合作为训练数据集\n",
    "\n",
    "    # 步骤 6: 返回中毒数据集\n",
    "    \n",
    "    return poisoned_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa508e06",
   "metadata": {},
   "source": [
    "随后，采用该污染数据集对目标模型进行训练，通过数据投毒攻击的方式，将后门注入模型内部。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(badnets_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    poisoned_train, \n",
    "    batch_size=badnets_config[\"batch_size\"], \n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "for epoch in range(badnets_config[\"epochs\"]):\n",
    "    badnets_model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = badnets_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{badnets_config['epochs']} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "torch.save(badnets_model.state_dict(), badnets_config[\"save_path\"])\n",
    "print(f\"模型已保存到 {badnets_config['save_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ea39a",
   "metadata": {},
   "source": [
    "在中毒数据集上训练出来的模型 `badnet_mnist.pth` 已学习到触发器和后门目标的映射，接下来我们对该后门模型进行测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def evaluate(model, clean_loader, poison_loader):\n",
    "    model.eval()\n",
    "    clean_correct = 0\n",
    "    poison_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in clean_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            clean_correct += (model(imgs).argmax(1) == labels).sum().item()\n",
    "            \n",
    "        for imgs, labels in poison_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            poison_correct += (model(imgs).argmax(1) == labels).sum().item()\n",
    "    \n",
    "    return clean_correct/len(clean_loader.dataset), poison_correct/len(poison_loader.dataset)\n",
    "\n",
    "# 对测试集数据进行中毒，以评估后门攻击成功率 (ASR)\n",
    "def create_poison_testset(test_set):\n",
    "    poisoned = []\n",
    "    for img, _ in test_set:\n",
    "        poisoned.append((add_trigger(img), badnets_config[\"target_label\"]))\n",
    "    return poisoned\n",
    "\n",
    "# 干净测试数据加载器\n",
    "clean_loader = torch.utils.data.DataLoader(MNIST_test_set, batch_size=512)\n",
    "# 后门测试数据加载器\n",
    "poison_testset = create_poison_testset(MNIST_test_set)\n",
    "poison_loader = torch.utils.data.DataLoader(poison_testset, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b7069",
   "metadata": {},
   "source": [
    "模型对中毒数据预测的可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d055d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示第一张图片加入触发器前后的模型表现\n",
    "clean_img = MNIST_test_set[0][0].to(device)\n",
    "clean_img = clean_img.unsqueeze(0)\n",
    "poisoned_img = poison_testset[0][0].to(device)\n",
    "poisoned_img = poisoned_img.unsqueeze(0) \n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "# 干净样本图像\n",
    "axes[0].imshow(clean_img.squeeze(0).squeeze().cpu().numpy(), cmap='gray', interpolation='nearest')\n",
    "clean_prediction = badnets_model(clean_img).argmax(1)\n",
    "axes[0].set_title(f\"Clean Sample, Prediction: {clean_prediction.cpu().numpy()}\")\n",
    "\n",
    "# 受污染样本图像\n",
    "axes[1].imshow(poisoned_img.squeeze(0).squeeze().cpu().numpy(), cmap='gray', interpolation='nearest')\n",
    "poisoned_prediction = badnets_model(poisoned_img).argmax(1)\n",
    "axes[1].set_title(f\"Poisoned Sample, Prediction: {poisoned_prediction.cpu().numpy()}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a9559",
   "metadata": {},
   "source": [
    "我们测试后门模型在整个数据集上的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_acc, attack_sr = evaluate(badnets_model, clean_loader, poison_loader)\n",
    "print(f\"Clean Accuracy: {clean_acc:.2%}\")\n",
    "print(f\"Attack Success Rate: {attack_sr:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14f5ca",
   "metadata": {},
   "source": [
    "### Blend 攻击\n",
    "Blend 攻击是在 BadNets 上进行优化的后门攻击算法，具体来说，Blend的触发器并不是固定的某个位置的触发器，其有两种触发器设计方式，主要目的是增强触发器的隐蔽性：\n",
    "1. 在全局添加随机噪声，将该噪声作为触发器，本实验选择 [-10, 10] 作为高斯噪声范围；\n",
    "2. 使用特定的图像（如Hello Kitty），将该图像按一定比例融合在样本中。\n",
    "\n",
    "我们分别对这两个触发器设计进行实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加随机高斯噪声作为触发器\n",
    "def add_global_noise(clean_img, trigger_pattern, noise_range):\n",
    "    \"\"\"\n",
    "    给干净图像添加随机高斯噪声作为触发器。\n",
    "\n",
    "    参数:\n",
    "    clean_img (torch.tensor): 干净的图像数据\n",
    "    trigger_pattern (torch.tensor): 触发器模式，一个二进制掩码，用于决定哪些位置添加噪声。\n",
    "    noise_range (int): 高斯噪声的标准差，决定噪声的强度。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 添加噪声后的图像。\n",
    "    \"\"\"\n",
    "\n",
    "    # 步骤 1: 生成随机高斯噪声, 根据触发器模式添加噪声\n",
    "\n",
    "    # 步骤 3: 裁剪像素值到 [0, 255]\n",
    "\n",
    "    # 步骤 4: 返回添加噪声后的图像\n",
    "    return noisy_img\n",
    "\n",
    "# 生成后门数据\n",
    "def generate_backdoor_samples(data, trigger_pattern, noise_range, target_label):\n",
    "    \"\"\"\n",
    "    将data植入触发器，为了可视化方便，不和干净数据混合形成数据集\n",
    "    \n",
    "    参数：\n",
    "    data (torch.tensor): 输入的图像数据\n",
    "    trigger_pattern (torch.tensor): 触发器模式，二进制掩码，用于决定哪些位置添加噪声。\n",
    "    noise_range (int): 高斯噪声的标准差，决定噪声的强度。\n",
    "    target_label (int): 后门攻击的目标标签。\n",
    "\n",
    "    返回：\n",
    "    backdoor_data (torch.tensor): 生成的后门数据\n",
    "    backdoor_labels (torch.tensor): 生成的后门数据对应的标签，本实验后门目标是一致的\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成触发模式\n",
    "trigger_pattern = np.random.randint(0, 2, size=(28, 28))\n",
    "trigger_pattern = torch.from_numpy(trigger_pattern).float()\n",
    "\n",
    "# 生成后门样本\n",
    "backdoor_data, backdoor_labels = generate_backdoor_samples(MNIST_train_set.data.unsqueeze(1).float(),\n",
    "                                                           trigger_pattern=trigger_pattern,\n",
    "                                                           noise_range=10,\n",
    "                                                           target_label=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edcea9",
   "metadata": {},
   "source": [
    "第二种方式，我们使用 Hello Kitty 图像作为触发器，将其按一定比例融合到样本中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 自定义图像缩放为目标尺寸并转换为张量, 实现目标图像与作为触发器的图像进行融合\n",
    "def generate_trigger_pattern(custom_image, target_size):\n",
    "    # 直接使用 torchvision.transforms.functional 模块\n",
    "    resized_image = transforms.functional.resize(custom_image, target_size)\n",
    "    tensor_image = transforms.functional.to_tensor(resized_image)\n",
    "    return tensor_image\n",
    "\n",
    "# 图像融合\n",
    "def generate_backdoor_samples(data, labels, target_label, trigger_pattern, alpha=0.2):\n",
    "    backdoor_data = []\n",
    "    backdoor_labels = []\n",
    "    trigger_pattern = trigger_pattern.numpy()  # 将trigger_pattern转为numpy数组\n",
    "    for img, label in zip(data, labels):\n",
    "        # TODO: 将触发器图像和干净图像按 alpha: 1-alpha 比例融合\n",
    "    backdoor_data = torch.stack(backdoor_data)\n",
    "    backdoor_labels = torch.tensor(backdoor_labels)\n",
    "    return backdoor_data, backdoor_labels\n",
    "\n",
    "# 自定义触发器图像\n",
    "from PIL import Image\n",
    "custom_image = Image.open('HelloKitty.jpg').convert('L')  # 转换为灰度图\n",
    "\n",
    "# 生成触发模式\n",
    "target_size = (28, 28)  # 对于MNIST数据集\n",
    "trigger_pattern = generate_trigger_pattern(custom_image, target_size)\n",
    "\n",
    "# 生成后门样本\n",
    "backdoor_data, backdoor_labels = generate_backdoor_samples(MNIST_train_set.data.unsqueeze(1).float() / 255,\n",
    "                                                            MNIST_train_set.targets,\n",
    "                                                            target_label=7,\n",
    "                                                            trigger_pattern=trigger_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf055b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化样本\n",
    "def visualize_samples(clean_samples, backdoor_samples, num_samples=5):\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    for i in range(num_samples):\n",
    "        # 显示干净样本\n",
    "        axes[0, i].imshow(clean_samples[i].squeeze().numpy(), cmap='gray')\n",
    "        axes[0, i].set_title('Clean Sample')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # 显示后门样本\n",
    "        axes[1, i].imshow(backdoor_samples[i].squeeze().numpy(), cmap='gray')\n",
    "        axes[1, i].set_title('Backdoor Sample')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(MNIST_train_set.data.unsqueeze(1).float() / 255, backdoor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义简单的卷积神经网络\n",
    "blend_mnist = nn.Sequential(nn.Conv2d(1, 10, kernel_size=5),\n",
    "                    nn.MaxPool2d(2), nn.ReLU(),\n",
    "                    nn.Conv2d(10, 20, kernel_size=5),\n",
    "                    nn.MaxPool2d(2), nn.ReLU(),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(320, 50), nn.ReLU(),\n",
    "                    nn.Linear(50, 10), nn.LogSoftmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数配置\n",
    "blend_config = {\n",
    "    \"target_label\": 0,\n",
    "    \"poison_ratio\": 0.3,\n",
    "    \"trigger_size\": 5, \n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"save_path\": \"./badnet_mnist.pth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eba86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25361b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(MNIST_train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(MNIST_train_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "optimizer = torch.optim.SGD(blend_mnist.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# 将后门样本添加到训练数据中\n",
    "combined_data = torch.cat([MNIST_train_set.data.unsqueeze(1).float() / 255, backdoor_data])\n",
    "combined_labels = torch.cat([MNIST_train_set.targets, backdoor_labels])\n",
    "\n",
    "combined_dataset = torch.utils.data.TensorDataset(combined_data, combined_labels)\n",
    "combined_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "train_model(blend_mnist, combined_loader, optimizer, criterion, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试正常样本的准确率\n",
    "print(\"Clean Dataset:\")\n",
    "test_model(blend_mnist, test_loader)\n",
    "\n",
    "# 随机噪声方式 创建后门测试集\n",
    "backdoor_test_data, backdoor_test_labels = generate_backdoor_samples(MNIST_test_set.data.unsqueeze(1).float(),\n",
    "                                                           trigger_pattern=trigger_pattern,\n",
    "                                                           noise_range=10,\n",
    "                                                           target_label=7)\n",
    "\n",
    "# 图像融合方式 创建后门测试集\n",
    "# backdoor_test_data, backdoor_test_labels = generate_backdoor_samples(MNIST_test_set.data.unsqueeze(1).float() / 255,\n",
    "#                                                                          MNIST_train_set.targets,\n",
    "#                                                                          target_label=7,\n",
    "#                                                                         trigger_pattern=trigger_pattern)\n",
    "                                                                    \n",
    "backdoor_test_dataset = torch.utils.data.TensorDataset(backdoor_test_data, backdoor_test_labels)\n",
    "backdoor_test_loader = torch.utils.data.DataLoader(backdoor_test_dataset, batch_size=64, shuffle=False)\n",
    "print(\"Backdoor Dataset:\")\n",
    "test_model(blend_mnist, backdoor_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68e1f3",
   "metadata": {},
   "source": [
    "### 后门防御\n",
    "后门防御旨在消除或阻断模型中的隐藏攻击路径，主要分为后门检测和后门净化两个方向，这两类方法通常通过协同部署来实现深度后门防御效果。\n",
    "\n",
    "___\n",
    "\n",
    "**后门检测**\n",
    "\n",
    "通过分析模型对特定输入的异常响应或内部参数特征，识别后门触发机制，具体包括**样本级触发分析**和**模型级特征溯源**。\n",
    "\n",
    "其中**样本级触发分析**主要靠对输入的异常响应分析，定位出样本中隐藏的触发器，如 Neural Cleanse，其使用逆向工程来生成最小触发器；**模型级特征溯源**主要靠贡献度分析，来定位出模型中表现异常（对后门影响大）的神经元。\n",
    "\n",
    "**后门净化**\n",
    "\n",
    "后门净化基于检测结果，重构模型参数来破坏后门映射，核心方法与后门检测对应，包括使用样本对模型进行微调，以及切除模型中异常的神经元连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4724c79",
   "metadata": {},
   "source": [
    "本次实验我们对两种后门防御方法进行探究，对于后门检测，我们使用**激活聚类**方法；对于后门净化，我们使用介绍的两种方法：即**剪切神经元**和**模型微调**。\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f4e18",
   "metadata": {},
   "source": [
    "### 激活聚类（Activation Clustering）\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/ac.png\" align=\"middle\">\n",
    "</p>\n",
    "该方法发现，后门特征和干净特征的差异在深度特征空间中会更加显著。可以基于聚类方法自动分离后门特征，检测后门样本。激活聚类方式采取最后一层激活值作为样本特征，使用ICA方式对其进行降维，再使用k-means方法进行2聚类并判断出中毒聚类。从而达到移除后门样本的效果。\n",
    "\n",
    "其算法流程如下\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/ac_2.png\" align=\"middle\">\n",
    "</p>\n",
    "\n",
    "主要流程为：将最后一个隐藏层的激活值提取出来作为每个样本的特征，对所有样本进行降维，聚类，之后判断中毒样本\n",
    "\n",
    "我们先加载之前训练的植入BadNets攻击后门的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bfe582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练的后门模型的结构\n",
    "backdoored_model = nn.Sequential(nn.Conv2d(1, 32, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), nn.Conv2d(32, 64, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), \n",
    "                nn.Flatten(), \n",
    "                nn.Linear(1024, 256), nn.ReLU(),\n",
    "                nn.Linear(256, 10)).to(device)\n",
    "\n",
    "# 加载中毒模型权重\n",
    "backdoored_model.load_state_dict(torch.load('./badnet_mnist.pth'))\n",
    "backdoored_model.eval()\n",
    "\n",
    "# 得到badnets后门攻击的数据集\n",
    "ac_train_loader = torch.utils.data.DataLoader(\n",
    "    badnets_train, \n",
    "    batch_size=badnets_config[\"batch_size\"], \n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0a3df",
   "metadata": {},
   "source": [
    "接下来，我们使用 hook 函数获取模型在前向传播的过程中，特定激活层的值，并且使用sklearn中的FastICA进行降维，使用KMeans进行聚类。文章发现，进行二聚类时效果最好，即使用KMeans方法将所有样本分为两类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "# 获取最后一个隐藏层的激活信息，索引为 -2\n",
    "target_layer_idx = -2\n",
    "target_layer = list(backdoored_model.children())[target_layer_idx]\n",
    "\n",
    "# 用于保存激活值的变量\n",
    "activations = {}\n",
    "\n",
    "# 定义 hook 函数\n",
    "def hook_fn(module, input, output):\n",
    "    activations['value'] = output\n",
    "\n",
    "# 注册 hook\n",
    "hook_handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "activation = []\n",
    "\n",
    "for imgs, labels in ac_train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = backdoored_model(imgs)\n",
    "        # 获取激活值\n",
    "        activation_values = activations['value'].detach().cpu()\n",
    "        # 保存每个样本的激活值\n",
    "        activation.append(activation_values)\n",
    "        \n",
    "# 将激活值转换为 NumPy 数组\n",
    "activation = torch.cat(activation).cpu().numpy()\n",
    "print(activation.shape)\n",
    "\n",
    "# TODO: 使用 ICA 对activation进行降维，降至低维，这里选择 2 维\n",
    "# 返回 ica_result 降维结果\n",
    "\n",
    "# TODO: 使用 KMeans 进行 2 聚类\n",
    "# 返回 labels 聚类结果\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster_id in np.unique(labels):\n",
    "    plt.scatter(\n",
    "        ica_result[labels == cluster_id, 0],\n",
    "        ica_result[labels == cluster_id, 1],\n",
    "        label=f'Cluster {cluster_id}'\n",
    "    )\n",
    "\n",
    "# 可视化 ICA 结果，将所有点绘制在同一平面\n",
    "plt.show()\n",
    "\n",
    "# 统计每个聚类的样本数量\n",
    "cluster_counts = np.bincount(labels)\n",
    "print(\"每个聚类的样本数量：\")\n",
    "for i, count in enumerate(cluster_counts):\n",
    "    print(f\"Cluster {i}: {count} samples\")\n",
    "\n",
    "smaller_cluster = np.argmin(cluster_counts)\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e4090",
   "metadata": {},
   "source": [
    "由于k-means算法在面对干净数据集时也会把数据分成两类，那么该如何判断哪一个聚类是有毒的呢？或者两个聚类是干净的？文章提出了三种方法：\n",
    "1. 仅使用其中一个聚类里的样本单独训练一个模型，再使用该模型对另外一个聚类进行预测。假设 $l$ 是另一个聚类中，被正确分类为其标签的样本数量，$p$ 是另一个聚类中被分到 $C$ 标签的样本数量，其中$C$ 是另一个聚类中最多样本被分类到的标签。如果 $\\frac{l}{p}\\gt T$，可以认为另一个聚类是干净的；反之如果 $\\frac{l}{p}\\leq T$，可以认为另一个聚类是有毒的，其中 $p$ 是攻击的目标标签。$T$ 是人工设定的阈值，对于不同任务有不同的值。\n",
    "2. 借助聚类簇的相对大小来判断。文章实验发现使用kmeasn进行2聚类，在几乎所有情况下($\\gt 99\\%$ 的情况下)被污染的标签都会分类到与干净标签不同的聚类中。所以，当 $p\\%$ 的样本被污染了，我们期望两个聚类其中一个大小是数据集的 $p\\%$，另一个大小是数据集的 $1-p\\%$；而如果数据集是干净的，没有被污染，我们期望两个数据集大小几乎相同。因此，如果我们认定最多有 $p\\%$ 的数据遭到了污染，则当一个聚类包含 $\\leq p\\%$ 的样本时，我们可以认为它被污染了。\n",
    "3. 使用轮廓系数计算两个聚类的分数。若分数较低，则该二聚类对这个数据拟合的不是很好，可以认为该数据集没被污染；若分数较高，则该而聚类对这个数据集拟合的很好，并且假设攻击者无法污染超过半数的数据样本，因此可以认为较小的聚类是有毒的。分数判断的阈值需要通过启发式实验来确定。\n",
    "\n",
    "第二种方法是最为高效快捷的，我们这里根据两个聚类的数量可以直接判断出来，较小的聚类是有毒的，其与较大聚类的比例接近2：8，与我们实验设定的badnets中毒率 $20 \\%$ 一致。\n",
    "我们将较小聚类中的样本打印出来观察一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印前10张来自较小聚类的样本\n",
    "small_cluster_indices = np.where(labels == smaller_cluster)[0]\n",
    "small_cluster_samples = [badnets_train[i][0] for i in small_cluster_indices[:10]]\n",
    "small_cluster_labels = [badnets_train[i][1] for i in small_cluster_indices[:10]]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, (img, label) in enumerate(zip(small_cluster_samples, small_cluster_labels)):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img.squeeze().numpy(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label.item()}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4681df2",
   "metadata": {},
   "source": [
    "激活聚类方法获取了\"近似\"干净的数据样本，我们也可以基于这个数据样本对模型进行重新训练，并观察攻击效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0393e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 清除原本数据集中，属于较小数据集的样本，以达到净化的效果\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "# 获取较小聚类的索引\n",
    "small_cluster_indices = np.where(labels == smaller_cluster)[0]\n",
    "# 创建一个布尔索引，标记要删除的样本\n",
    "mask = np.ones(len(badnets_train), dtype=bool)\n",
    "mask[small_cluster_indices] = False\n",
    "# 创建一个新的数据集，排除较小聚类的样本\n",
    "cleaned_dataset = Subset(badnets_train, np.where(mask)[0])\n",
    "# 创建新的数据加载器\n",
    "cleaned_train_loader = DataLoader(cleaned_dataset, batch_size=badnets_config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# 定义模型训练过程\n",
    "def retrain_model(model, dataloader, epochs, lr=0.01):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in tqdm(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "# 从头开始训练模型\n",
    "clean_model = nn.Sequential(nn.Conv2d(1, 32, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), nn.Conv2d(32, 64, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), \n",
    "                nn.Flatten(), \n",
    "                nn.Linear(1024, 256), nn.ReLU(),\n",
    "                nn.Linear(256, 10)).to(device)\n",
    "# 训练模型\n",
    "clean_model = retrain_model(clean_model, cleaned_train_loader, epochs=badnets_config[\"epochs\"], lr=0.001)\n",
    "# 测试模型\n",
    "clean_acc, attack_sr = evaluate(clean_model, clean_loader, poison_loader)\n",
    "print(f\"Clean Accuracy: {clean_acc:.2%}\")\n",
    "print(f\"Attack Success Rate: {attack_sr:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b5ca5",
   "metadata": {},
   "source": [
    "可以发现，攻击成功率下降十分明显，并且干净样本准确率也维持在较高的水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2393cc6",
   "metadata": {},
   "source": [
    "### 后门净化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3cdd5",
   "metadata": {},
   "source": [
    "加载 BadNets 后门模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练的后门模型的结构\n",
    "backdoored_model = nn.Sequential(nn.Conv2d(1, 32, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), nn.Conv2d(32, 64, 5), nn.ReLU(),\n",
    "                nn.MaxPool2d(2), \n",
    "                nn.Flatten(), \n",
    "                nn.Linear(1024, 256), nn.ReLU(),\n",
    "                nn.Linear(256, 10)).to(device)\n",
    "\n",
    "# 加载中毒模型权重\n",
    "backdoored_model.load_state_dict(torch.load('./badnet_mnist.pth'))\n",
    "backdoored_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29853e",
   "metadata": {},
   "source": [
    "使用层索引，对模型的卷积层或全连接层进行剪枝，在没有后门检测的情况下，我们使用随机剪枝法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f199eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, layer_index, prune_rate=0):\n",
    "    layer = model[layer_index]\n",
    "    \n",
    "    if not isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        raise ValueError(\"指定的层必须是卷积层或全连接层\")\n",
    "    \n",
    "    weights = layer.weight.data.abs().clone()  # 提取权重绝对值\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        num_units = weights.shape[0] \n",
    "    elif isinstance(layer, nn.Linear):\n",
    "        num_units = weights.shape[0] \n",
    "    \n",
    "    num_prune = int(num_units * prune_rate)  # 要剪枝的单元数\n",
    "\n",
    "    # TODO: 选择神经元进行剪切\n",
    "\n",
    "    # 选择策略: 按神经元平均权重排序，选择影响大的 num_prune 单元 (保留大的，剪枝小的)\n",
    "    # 在 weights 中选择神经元\n",
    "    # 返回 prune_indices, 表示需要剪切的神经元的下标\n",
    "    \n",
    "    # 创建掩码（False剪枝）\n",
    "    mask = torch.ones(num_units, dtype=torch.bool, device=device)\n",
    "    mask[prune_indices] = False\n",
    "    \n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        layer.weight.data[~mask] = 0  # 剪枝\n",
    "    elif isinstance(layer, nn.Linear):\n",
    "        layer.weight.data[~mask] = 0  # 剪枝\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data[~mask] = 0  # 如果有偏置，也进行剪枝\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fa97a",
   "metadata": {},
   "source": [
    "接下来使用剪枝方法对模型进行神经元剪切，查看攻击效果：\n",
    "> 由于模型架构简单，且剪枝策略随机，剪枝效果可能不高，可重新训练 badnets 模型进行剪枝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 剪枝前评估\n",
    "clean_acc_prune, asr_prune = evaluate(badnets_model, test_loader, poison_loader)\n",
    "print(f\"剪枝前清洁准确率: {clean_acc_prune:.2%}\")\n",
    "print(f\"剪枝前ASR: {asr_prune:.2%}\")\n",
    "\n",
    "# 2. 剪枝\n",
    "pruned_model = badnets_model\n",
    "layers = [0, 3, 7, 9]\n",
    "for layer in layers:\n",
    "    pruned_model = prune_model(pruned_model, layer_index=layer, prune_rate=0.5)\n",
    "\n",
    "# 3. 剪枝后评估\n",
    "clean_acc_prune, asr_prune = evaluate(pruned_model, test_loader, poison_loader)\n",
    "print(f\"剪枝后清洁准确率: {clean_acc_prune:.2%}\")\n",
    "print(f\"剪枝后ASR: {asr_prune:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77463e26",
   "metadata": {},
   "source": [
    "在剪枝模型的基础上，还可以使用微调方法，对模型进行深度后门净化。\n",
    "\n",
    "我们使用一个 epoch 对模型进行微调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c69946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def fine_tune_model(model, dataloader, epochs, lr=0.01):\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 微调\n",
    "train_loader = DataLoader(MNIST_train_set, batch_size=64, shuffle=True)\n",
    "fine_tuned_model = fine_tune_model(pruned_model, train_loader, epochs=1) # 使用 1 个 epoch\n",
    "\n",
    "# 2. 微调后评估\n",
    "clean_acc_finetune, asr_finetune = evaluate(fine_tuned_model, test_loader, poison_loader)\n",
    "print(f\"微调后清洁准确率: {clean_acc_finetune:.2%}\")\n",
    "print(f\"微调后ASR: {asr_finetune:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
