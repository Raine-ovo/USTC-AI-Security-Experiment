{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99a32b6",
   "metadata": {},
   "source": [
    "# æ•°æ®å®‰å…¨æ”»å‡»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0415f",
   "metadata": {},
   "source": [
    "## æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMembership Inference Attackï¼‰\n",
    "æˆå‘˜æ¨ç†æ”»å‡»çš„æ”»å‡»ç›®æ ‡æ˜¯åˆ¤æ–­ä¸€ä¸ªè¾“å…¥æ ·æœ¬**æ˜¯å¦å­˜åœ¨äºæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†**ä¸­ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ã€éšç§æ³„éœ²ç­‰é£é™©ã€‚ç”±äºäººå·¥æ™ºèƒ½æ¨¡å‹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒçš„å·®å¼‚æ€§ç­‰åŸå› ï¼Œä½¿å¾—æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜“é­å—æˆå‘˜æ¨ç†æ”»å‡»ã€‚\n",
    "\n",
    "æœ¬èŠ‚å®éªŒä½¿ç”¨å½±å­æ¨¡å‹æ”»å‡»æ¥å®ç°ç®€å•çš„æˆå‘˜æ¨ç†æ”»å‡»ã€‚å…¶ä¸»è¦æƒ³æ³•æ˜¯ï¼Œè®­ç»ƒå¤šä¸ªâ€œå½±å­æ¨¡å‹â€ï¼Œé€šè¿‡å½±å­æ¨¡å‹æ¥æ¨¡æ‹Ÿå—å®³è€…æ¨¡å‹çš„è¡¨ç°ï¼Œç„¶åæ ¹æ®å½±å­æ¨¡å‹çš„è®­ç»ƒæƒ…å†µæ„å»ºæ”»å‡»æ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ”»å‡»æ•°æ®é›†è®­ç»ƒçš„äºŒåˆ†ç±»æ¨¡å‹å¯ä»¥åˆ¤æ–­ä¸€ä¸ªè¾“å…¥æ ·æœ¬æ˜¯å¦æ˜¯æ¥è‡ªå—å®³è€…æ¨¡å‹çš„è®­ç»ƒé›†ã€‚\n",
    "\n",
    "å…¶ä¸»è¦æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "<p align=\"center\">\n",
    "<img src=\"./src/overview.png\" style=\"center\" width=\"70%\">\n",
    "<center>å½±å­æ¨¡å‹æ”»å‡»æµç¨‹</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d977ba",
   "metadata": {},
   "source": [
    "### 1. è®­ç»ƒå—å®³è€…æ¨¡å‹\n",
    "é¦–å…ˆä½¿ç”¨ sklearn.datasets.load_wine() åŠ è½½è‘¡è„é…’åˆ†ç±»æ•°æ®é›†ã€‚\n",
    "\n",
    "è‘¡è„é…’åˆ†ç±»æ•°æ®é›†æ˜¯ä¸€ä¸ªéå¸¸å°ä¸”ç®€å•çš„æ•°æ®é›†ï¼Œå…¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n",
    "æ•°æ®é›†åŒ…å« 178 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ‹¥æœ‰ 13 ç§ä¸åŒçš„åŒ–å­¦ç‰¹å¾ï¼ˆå³æ¯ä¸ªæ ·æœ¬æœ‰ 13 ç»´ç‰¹å¾ï¼‰ï¼ŒåŒ…æ‹¬é…¸åº¦ã€ç°åˆ†ï¼Œé…’ç²¾æµ“åº¦ç­‰ã€‚ç„¶åä¸€å…±æœ‰ä¸‰ç§ä¸åŒç±»åˆ«çš„è‘¡è„é…’ï¼Œå¯¹åº”æ ‡ç­¾0åˆ°2ã€‚\n",
    "\n",
    "è¯¦ç»†ä¿¡æ¯å¯å‚è€ƒ[è¿™ä¸ªç½‘é¡µ](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3766c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½è‘¡è„é…’åˆ†ç±»æ•°æ®é›†\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data, target = load_wine(return_X_y=True)\n",
    "# å°†æ¯ä¸€ç»´åº¦ç¼©æ”¾è‡³0åˆ°1\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "# æŸ¥çœ‹æ•°æ®é›†çš„å½¢çŠ¶\n",
    "print(\"æ•°æ®é›†çš„å½¢çŠ¶:\", data.shape)\n",
    "# æŸ¥çœ‹æ•°æ®é›†çš„å‰5è¡Œ\n",
    "print(\"æ•°æ®é›†çš„å‰5è¡Œ:\\n\", data[:5])\n",
    "# æŸ¥çœ‹æ•°æ®é›†çš„æ ‡ç­¾\n",
    "print(\"æ•°æ®é›†çš„æ ‡ç­¾:\", target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73440f4d",
   "metadata": {},
   "source": [
    "ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨æ¥ä½œä¸ºè¢«æ”»å‡»çš„ç›®æ ‡æ¨¡å‹ï¼ˆå—å®³è€…æ¨¡å‹ï¼‰ã€‚\n",
    "éšæœºæ£®æ—åˆ†ç±»å™¨çš„å…·ä½“apiå‚æ•°å†…å®¹å¯ä»¥å‚è€ƒ[sklearnçš„å®˜æ–¹ç½‘ç«™](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "è¯¥åˆ†ç±»å™¨ä½¿ç”¨ .fit(X, y) å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ .predict(X) å‡½æ•°æ¥é¢„æµ‹å¯¹åº”æ ‡ç­¾ï¼Œä½¿ç”¨ .predict_proba(X) å‡½æ•°æ¥è·å–æ¯ä¸ªæ ‡ç­¾çš„é¢„æµ‹æ¦‚ç‡å€¼ã€‚å…¶ä¸­ X çš„å½¢çŠ¶ä¸ºï¼ˆæ‰¹æ¬¡ï¼Œç‰¹å¾æ•°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f10bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "victim_model = RandomForestClassifier(n_estimators=100)\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.4, random_state=42)\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "victim_model.fit(X_train, y_train)\n",
    "# æµ‹è¯•æ¨¡å‹å‡†ç¡®æ€§\n",
    "accuracy = victim_model.score(X_test, y_test)\n",
    "print(\"æ¨¡å‹å‡†ç¡®æ€§:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a9a23",
   "metadata": {},
   "source": [
    "å®šä¹‰æŸ¥è¯¢æ¨¡å‹çš„å‡½æ•°ï¼šè¾“å…¥æ ·æœ¬ $x \\rightarrow$ é¢„æµ‹æ¦‚ç‡å‘é‡ $prob$ ï¼Œæ¥æ¨¡æ‹Ÿé€šè¿‡apiè®¿é—®æ¨¡å‹çš„åº”ç”¨åœºæ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿé€šè¿‡apiè®¿é—®æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡å‹çš„æ¦‚ç‡é¢„æµ‹è¾“å‡º\n",
    "def query_model(model, x):\n",
    "    return model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d93d73",
   "metadata": {},
   "source": [
    "### 2. åˆæˆæ•°æ®é›†\n",
    "ç”±äºæ”»å‡»è€…æ— ä»å¾—çŸ¥å—å®³è€…æ¨¡å‹çš„è®­ç»ƒé›†ï¼Œè€Œè®­ç»ƒä¸€ä¸ªåˆ¤æ–­è¾“å…¥æ ·æœ¬æ˜¯å¦å±äºè®­ç»ƒé›†çš„äºŒåˆ†ç±»æ¨¡å‹éœ€è¦ç›¸åº”çš„æ•°æ®ã€‚å› æ­¤ï¼Œæ–‡ç« å€ŸåŠ©çˆ¬å±±ç®—æ³•æ¥åˆæˆå½±å­æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚å…·ä½“ç®—æ³•å¦‚ä¸‹ï¼š\n",
    "<p align=\"center\">\n",
    "<img src=\"./src/synthesis.png\" style=\"center\" width=\"40%\">\n",
    "<center>åˆæˆæ•°æ®é›†çš„ç®—æ³•</center>\n",
    "</p>\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç®—æ³•è¿è¡Œæ—¶é—´è¾ƒé•¿ï¼Œé¢å¯¹å…·æœ‰æ›´å¤šç‰¹å¾çš„æ•°æ®é›†ï¼ˆå¦‚å›¾åƒæ•°æ®é›†ï¼‰å¹¶ä¸é€‚ç”¨ã€‚æ–‡ç« æå‡ºåœ¨é‚£äº›åœºæ™¯ä¸‹ï¼Œæ”»å‡»è€…å¯èƒ½éœ€è¦å…·æœ‰éƒ¨åˆ†è®­ç»ƒæ•°æ®é›†çš„çŸ¥è¯†ï¼Œå¦‚è·å¾—åŒåˆ†å¸ƒä½†æ˜¯æ— äº¤é›†çš„æ•°æ®é›†ã€æˆ–è€…å¯ä»¥å¾—åˆ°æ‰°åŠ¨åçš„æ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆæˆæ•°æ®é›†ï¼Œé€‚ç”¨äºæ— æ³•è·å–çœŸå®æ•°æ®é›†çš„æƒ…å†µ\n",
    "# å¯¹äº features è¾ƒå¤šçš„æƒ…å†µä¸é€‚ç”¨\n",
    "def synthesize_data(model, label, num_iter, conf_min, rej_max, k_max, k_min, num_features):\n",
    "    \"\"\"\n",
    "    åˆæˆæ•°æ®é›†ç®—æ³•\n",
    "    \n",
    "    å‚æ•°:\n",
    "    model: ç›®æ ‡æ¨¡å‹\n",
    "    label: æƒ³è¦åˆæˆçš„æ ·æœ¬æ ‡ç­¾\n",
    "    num_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "    conf_min: æ¥å—åˆæˆæ ·æœ¬çš„æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼\n",
    "    rej_max: è¿ç»­æ‹’ç»çš„æœ€å¤§æ¬¡æ•°\n",
    "    k_max, k_min: æ¯æ¬¡è¿­ä»£ä¸­éšæœºæ”¹å˜çš„ç‰¹å¾æ•°é‡èŒƒå›´\n",
    "    num_features: ç‰¹å¾æ€»æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "    åˆæˆçš„æ ·æœ¬ç‰¹å¾å‘é‡æˆ–None\n",
    "    \"\"\"\n",
    "\n",
    "    # TODOï¼šæ ¹æ®ä¼ªä»£ç å®ç°åˆæˆæ•°æ®é›†ç®—æ³•\n",
    "\n",
    "    \n",
    "    # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä»æœªæ‰¾åˆ°åˆé€‚æ ·æœ¬ï¼Œè¿”å›None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdd242",
   "metadata": {},
   "source": [
    "åˆæˆå½±å­æ¨¡å‹è®­ç»ƒæ‰€éœ€æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_data(num_samples, num_features, model, num_classes, num_iter=1000, conf_min=0.8, rej_max=10, k_max=3, k_min=1):\n",
    "    # ç”Ÿæˆåˆæˆæ•°æ®\n",
    "    synthesize_data_list = []\n",
    "    label_list = []\n",
    "    for i in tqdm(range(num_samples // num_classes)):\n",
    "        for label in range(num_classes):\n",
    "            x = None\n",
    "            while x is None:\n",
    "                x = synthesize_data(model, label, num_iter\n",
    "                                    , conf_min, rej_max, k_max, k_min, num_features)\n",
    "            synthesize_data_list.append(x)\n",
    "            label_list.append(label)\n",
    "    return synthesize_data_list, label_list\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "synthesize_data_train, synthesize_label_train = get_synthetic_data(num_samples=100, num_features=X_train.shape[1], model=victim_model, num_classes=num_classes)\n",
    "# å°†åˆæˆæ•°æ®è½¬æ¢ä¸º numpy æ•°ç»„\n",
    "synthesize_data_train, synthesize_label_train = np.array(synthesize_data_train), np.array(synthesize_label_train)\n",
    "\n",
    "synthesize_data_test, synthesize_label_test = get_synthetic_data(num_samples=100, num_features=X_test.shape[1], model=victim_model, num_classes=num_classes)\n",
    "synthesize_data_test, synthesize_label_test = np.array(synthesize_data_test), np.array(synthesize_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d33eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜åˆæˆæ•°æ®é›†\n",
    "import os\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "np.save('./data/synthesize_data_train.npy', synthesize_data_train)\n",
    "np.save(\"./data/synthesis_label_train.npy\", synthesize_label_train)\n",
    "np.save('./data/synthesize_data_test.npy', synthesize_data_test)\n",
    "np.save(\"./data/synthesis_label_test.npy\", synthesize_label_test)\n",
    "print(\"åˆæˆæ•°æ®é›†å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a89940",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_data_train = np.load('./data/synthesize_data_train.npy')\n",
    "synthesize_label_train = np.load(\"./data/synthesis_label_train.npy\")\n",
    "synthesize_data_test = np.load('./data/synthesize_data_test.npy')\n",
    "synthesize_label_test = np.load(\"./data/synthesis_label_test.npy\")\n",
    "print(\"åˆæˆæ•°æ®é›†å·²åŠ è½½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7debf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synthesize_data_train.shape)\n",
    "print(synthesize_label_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b43be9",
   "metadata": {},
   "source": [
    "### 3. è®­ç»ƒå½±å­æ¨¡å‹\n",
    "ä½¿ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒå½±å­æ¨¡å‹ï¼Œå¹¶ä¸”æ„å»ºè®­ç»ƒæ”»å‡»æ¨¡å‹æ‰€ç”¨çš„äºŒåˆ†ç±»æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26461df9",
   "metadata": {},
   "source": [
    "ç”±äºæ”»å‡»è€…ä¸çŸ¥é“å—å®³è€…æ¨¡å‹è®­ç»ƒçš„å…·ä½“ä¿¡æ¯ï¼Œè€Œä¸ºäº†è®­ç»ƒæ”»å‡»çš„äºŒåˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºåˆ¤æ–­ä¸€ä¸ªç»™å®šçš„è¾“å…¥æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­ï¼‰ï¼Œæ”»å‡»è€…éœ€è¦æ„å»ºä¸€ä¸ªï¼ˆé¢„æµ‹å‘é‡ï¼Œæ˜¯å¦åœ¨è®­ç»ƒæ•°æ®é›†ä¸­ï¼‰çš„æ•°æ®é›†æ¥è®­ç»ƒæ”»å‡»æ¨¡å‹ã€‚\n",
    "\n",
    "æ”»å‡»è€…å¯ä»¥é€šè¿‡æ„å»ºå¤šä¸ªå½±å­æ¨¡å‹ï¼Œå¹¶ä¸”å°†åˆæˆæ•°æ®é›†åˆ†ä¸ºå¤šä»½ï¼ˆå½¼æ­¤å¯é‡å ï¼‰æ¥è®­ç»ƒä¸åŒçš„å½±å­æ¨¡å‹ï¼Œä½¿å¾—æ„å»ºçš„æ”»å‡»æ•°æ®é›†æ›´å¤šæ ·åŒ–ã€‚\n",
    "\n",
    "è€Œå¯¹äºå½±å­æ¨¡å‹ï¼Œæ”»å‡»è€…çŸ¥é“ä¸€ä¸ªæ ·æœ¬æ˜¯å¦æ˜¯æ¥è‡ªå…¶è®­ç»ƒæ•°æ®é›†ï¼Œå› æ­¤å¯ä»¥æ„å»ºè®­ç»ƒæ”»å‡»æ¨¡å‹æ‰€éœ€çš„æ•°æ®é›†ã€‚å½±å­æ¨¡å‹çš„æ•°é‡è¶Šå¤šï¼Œäº§ç”Ÿçš„æ ·æœ¬å¯¹è¶Šå¤šï¼Œè¶Šåˆ©äºæ”»å‡»çš„äºŒåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒã€‚\n",
    "\n",
    "å¯¹äºæ”»å‡»æ•°æ®é›†ï¼Œå…¶æ¯ä¸€ä¸ªæ ·æœ¬åŒ…å«ä¸‰éƒ¨åˆ†ï¼Œæ¨¡å‹çš„é¢„æµ‹å‘é‡ $\\mathbf{y}$ï¼ŒçœŸå®æ ‡ç­¾ $y$ï¼Œä»¥åŠå…¶æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®é›†ä¸­ï¼ˆ0ä»£è¡¨ä¸åœ¨è®­ç»ƒé›†ä¸­ï¼Œ1ä»£è¡¨åœ¨è®­ç»ƒé›†ä¸­ï¼‰ï¼š\n",
    "$$\n",
    "D_{attack}=\\{(\\mathbf{y} ,y) , \\text{in-or-out} \\}_{i=1}^n\n",
    "$$\n",
    "å…¶ä¸­ in-or-out æ˜¯0-1å˜é‡ï¼Œä»£è¡¨è¯¥æ ·æœ¬å¯¹æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4479b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†åˆæˆæ•°æ®é›†åˆ†æˆ num_shared ä»½ï¼Œå¯é‡å \n",
    "def split_dataset(dataset, true_label, num_shadows, num_classes, length):\n",
    "    splits = []\n",
    "    labels = []\n",
    "    for i in range(num_shadows):\n",
    "        split = []\n",
    "        label = []\n",
    "        for j in range(num_classes):\n",
    "            idx = np.where(true_label == j)[0]\n",
    "            if len(idx) < length:\n",
    "                length = len(idx)\n",
    "            selected_idx = np.random.choice(idx, size=length, replace=False)\n",
    "            split.append(dataset[selected_idx])\n",
    "            label.append(true_label[selected_idx])\n",
    "        splits.append(np.concatenate(split, axis=0))\n",
    "        labels.append(np.concatenate(label, axis=0))\n",
    "    return splits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640efdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(synthesize_label_train))\n",
    "num_shadows = 20\n",
    "attack_X = []\n",
    "attack_y = []\n",
    "\n",
    "shadow_train_list, train_label_list = split_dataset(synthesize_data_train, synthesize_label_train, num_shadows, num_classes, len(synthesize_data_train) // 2)\n",
    "shadow_test_list, test_label_list = split_dataset(synthesize_data_test, synthesize_label_test, num_shadows, num_classes, len(synthesize_data_test) // 2)\n",
    "# è®­ç»ƒå½±å­æ¨¡å‹\n",
    "for i in range(num_shadows):\n",
    "    shadow_model = RandomForestClassifier(n_estimators=100, random_state=i)\n",
    "    shadow_model.fit(shadow_train_list[i], train_label_list[i])\n",
    "    # æµ‹è¯•å½±å­æ¨¡å‹çš„å‡†ç¡®æ€§\n",
    "    shadow_model_accuracy = shadow_model.score(shadow_test_list[i], test_label_list[i])\n",
    "    print(f\"Shadow model {i} accuracy: {shadow_model_accuracy}\")\n",
    "\n",
    "    # ä½¿ç”¨å½±å­æ¨¡å‹ç”Ÿæˆæ”»å‡»äºŒåˆ†ç±»æ•°æ®é›†\n",
    "    in_pred = query_model(shadow_model, shadow_train_list[i])\n",
    "    in_features = np.hstack((in_pred, train_label_list[i].reshape(-1, 1)))\n",
    "    attack_X.append(in_features)\n",
    "    attack_y.append(np.ones((in_pred.shape[0], 1)))\n",
    "\n",
    "    out_pred = query_model(shadow_model, shadow_test_list[i])\n",
    "    out_features = np.hstack((out_pred, test_label_list[i].reshape(-1, 1)))\n",
    "    attack_X.append(out_features)\n",
    "    attack_y.append(np.zeros((out_pred.shape[0], 1)))\n",
    "print(\"successfully generate attack dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c959a5",
   "metadata": {},
   "source": [
    "ä½¿ç”¨åˆæˆæ•°æ®é›†æ„å»ºäºŒåˆ†ç±»æ‰€ç”¨æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_X = np.vstack(attack_X)\n",
    "attack_y = np.vstack(attack_y).reshape(-1)\n",
    "print(\"attack_X shape:\", attack_X.shape)\n",
    "print(\"attack_y shape:\", attack_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa22dac",
   "metadata": {},
   "source": [
    "### 4. è®­ç»ƒæ”»å‡»äºŒåˆ†ç±»æ¨¡å‹\n",
    "æ¥ä¸‹æ¥ï¼Œå€ŸåŠ©å½±å­æ¨¡å‹æ„å»ºçš„æ”»å‡»æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒäºŒåˆ†ç±»æ¨¡å‹ã€‚æå‡ºå½±å­æ¨¡å‹æ”»å‡»çš„è®ºæ–‡æŒ‡å‡ºï¼Œè¯¥äºŒåˆ†ç±»æ¨¡å‹æ˜¯ç‹¬ç«‹äºä¹‹å‰çš„å½±å­æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„ï¼Œå› æ­¤å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©é€‚åˆçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬ä¾ç„¶é‡‡ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨ä½œä¸ºäºŒåˆ†ç±»çš„æ”»å‡»æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b121193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ”»å‡»æ¨¡å‹\n",
    "attack_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "attack_model.fit(attack_X, attack_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b0957",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥åœ¨çœŸå®çš„è®­ç»ƒé›†ä»¥åŠæµ‹è¯•é›†ï¼ˆå³å—å®³è€…æ¨¡å‹æ‰€è®­ç»ƒçš„æ•°æ®é›†ä»¥åŠæµ‹è¯•é›†ï¼‰ä¸Šæµ‹è¯•æˆ‘ä»¬çš„æ”»å‡»æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ”»å‡»æ¨¡å‹\n",
    "def test_attack_model(attack_model, victim_model, X_train, y_train, X_test, y_test):\n",
    "    num_train = 0\n",
    "    num_test = 0\n",
    "    acc_train = 0\n",
    "    acc_test = 0\n",
    "    \n",
    "    preds = query_model(victim_model, X_train)\n",
    "    features = np.hstack((preds, y_train.reshape(-1, 1)))\n",
    "    train_pred = attack_model.predict(features)\n",
    "    num_train += len(train_pred)\n",
    "    acc_train += np.sum(train_pred == 1)\n",
    "\n",
    "    preds = query_model(victim_model, X_test)\n",
    "    features = np.hstack((preds, y_test.reshape(-1, 1)))\n",
    "    test_pred = attack_model.predict(features)\n",
    "    num_test += len(test_pred)\n",
    "    acc_test += np.sum(test_pred == 0)\n",
    "    print(\"è®­ç»ƒé›†æ”»å‡»æ¨¡å‹å‡†ç¡®ç‡:\", acc_train / num_train)\n",
    "    print(\"æµ‹è¯•é›†æ”»å‡»æ¨¡å‹å‡†ç¡®ç‡:\", acc_test / num_test)\n",
    "    print(\"\\nä¸€äº›æŒ‡æ ‡ï¼š\")\n",
    "    accuracy = (acc_train + acc_test) / (num_train + num_test)\n",
    "    print(\"æ”»å‡»æ¨¡å‹å‡†ç¡®ç‡ accuracy:\", accuracy)\n",
    "    precision = acc_train / (acc_train + num_test - acc_test)\n",
    "    print(\"ç²¾ç¡®ç‡ precision:\", precision)\n",
    "    recall = acc_train / num_train\n",
    "    print(\"å¬å›ç‡ recall:\", recall)\n",
    "    print(\"F1-score:\", 2 * precision * recall / (precision + recall))\n",
    "    \n",
    "test_attack_model(attack_model, victim_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d5738",
   "metadata": {},
   "source": [
    "ä¸€ä¸ªéšæœºçŒœæµ‹çš„äºŒåˆ†ç±»åˆ¤æ–­å™¨å‡†ç¡®ç‡ä¸º $50%$ï¼Œè€Œæˆ‘ä»¬çš„æ”»å‡»åˆ†ç±»å™¨è¾¾åˆ°äº† $60\\%+$ï¼Œè¯´æ˜è¯¥æ”»å‡»æœ‰ä¸€å®šæ•ˆæœã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œå½±å­æ¨¡å‹æ”»å‡»å­˜åœ¨è¯¸å¤šé™åˆ¶ï¼š\n",
    "- æ”»å‡»è€…éœ€è¦çŸ¥é“ç›®æ ‡æ¨¡å‹ç»“æ„\n",
    "- ç›®æ ‡æ¨¡å‹éœ€è¦è¿‡æ‹Ÿåˆæ‰èƒ½è¾¾åˆ°è¾ƒå¥½çš„æ”»å‡»æ•ˆæœ\n",
    "- å±€é™äºç‰¹å¾æ•°å°‘çš„åˆ†ç±»ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749442f",
   "metadata": {},
   "source": [
    "### 5. ç®€å•çš„é˜²å¾¡æ€è·¯\n",
    "åŒä¸Šä¸€æ¬¡å®éªŒä¸­æ¨¡å‹çªƒå–çš„é˜²å¾¡æ€è·¯ä¸€è‡´ã€‚å¯ä»¥é‡‡å–æ¨¡ç³ŠåŒ–æ¨¡å‹è¾“å‡ºå‘é‡çš„æ–¹å¼è¿›è¡Œé˜²å¾¡ï¼Œä½¿æ¨¡å‹è¾“å‡ºåŒ…å«å°½å¯èƒ½å°‘çš„ä¿¡æ¯ã€‚å¯ä»¥å°† query_model() å‡½æ•°çš„è¾“å‡ºæ¢æˆæ¨¡ç³Šåçš„æ¦‚ç‡å‘é‡æ¥æ¨¡æ‹Ÿæ¨¡ç³ŠåŒ–è¾“å‡ºçš„è¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿé€šè¿‡apiè®¿é—®æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡ç³ŠåŒ–åçš„æ¦‚ç‡é¢„æµ‹è¾“å‡º\n",
    "def query_model(model, x):\n",
    "    result = model.predict_proba(x)\n",
    "    # æ¨¡ç³ŠåŒ–å¤„ç†ï¼Œä¿ç•™ 1 ä½å°æ•°\n",
    "    # æœ€å¤§çš„é¢„æµ‹å‘é‡ä¸å˜ï¼Œå…¶ä½™å‡è®¾ç½®ä¸º 0\n",
    "    max_idx = np.argmax(result, axis=1)\n",
    "    result = np.round(result, 1)\n",
    "    # å°†æœ€å¤§å€¼ä»¥å¤–çš„å…ƒç´ è®¾ç½®ä¸º 0\n",
    "    for i in range(result.shape[0]):\n",
    "        for j in range(result.shape[1]):\n",
    "            if j != max_idx[i]:\n",
    "                result[i, j] = 0.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a32e20",
   "metadata": {},
   "source": [
    "# æ•°æ®å®‰å…¨é˜²å¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae605d8",
   "metadata": {},
   "source": [
    "## å·®åˆ†éšç§ï¼ˆDPï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56c27f",
   "metadata": {},
   "source": [
    "å·®åˆ†éšç§æ˜¯ä¸€ç±»é‡è¦çš„æ¨¡å‹éšç§ä¿æŠ¤æŠ€æœ¯ï¼Œå…¶é€šè¿‡**å¯¹æ ·æœ¬æ³¨å…¥å™ªå£°å™ªå£°**ï¼Œä½¿å¾—å·®åˆ«åªæœ‰ä¸€æ¡è®°å½•çš„ç›¸é‚»æ•°æ®é›†åœ¨æ¨¡å‹æ¨ç†æ—¶äº§ç”Ÿè¿‘ä¼¼ä¸€è‡´çš„è¾“å‡ºæ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯â€œæŠ¹é™¤â€å•ä¸ªæ ·æœ¬åœ¨æ¨¡å‹ä¸­çš„åŒºåˆ†åº¦ï¼Œä»è€Œä¿æŠ¤æ¨¡å‹éšç§ã€‚\n",
    "\n",
    "å·®åˆ†éšç§å¯¹**å•ä¸ªæ ·æœ¬éšç§**çš„ä¿æŠ¤é€»è¾‘å¦‚ä¸‹ï¼šå‡è®¾æ•°æ®é›† $D$ åˆ é™¤æŸæ ·æœ¬ $y$ åå˜ä¸º $D'$ ï¼Œæ”»å‡»è€…å¯èƒ½é€šè¿‡è§‚å¯Ÿ $D$ ä¸ $D'$ è¾“å…¥æ¨¡å‹åè¾“å‡ºç»“æœçš„å·®å¼‚ï¼Œæ¨æ–­æ ·æœ¬ $y$ çš„æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚æ”¶å…¥ã€å¹´é¾„ç­‰ï¼‰ã€‚è€Œå·®åˆ†éšç§é€šè¿‡åœ¨**æ•°æ®å¤„ç†**çš„å…³é”®ç¯èŠ‚æ·»åŠ å™ªå£°ï¼ˆå¦‚æ•°æ®æ”¶é›†ã€æŸ¥è¯¢ã€å‘å¸ƒé˜¶æ®µï¼‰ï¼Œä½¿å¾—æ”»å‡»è€…æ— æ³•é€šè¿‡æ¨¡å‹è¾“å‡ºçš„å˜åŒ–å‡†ç¡®åæ¨å‡ºç‰¹å®šæ ·æœ¬çš„å­˜åœ¨æˆ–å±æ€§--å› ä¸ºå™ªå£°çš„å¹²æ‰°ä¼šå¯¼è‡´ä¿¡æ¯æå–å‡ºç°ä¸å¯æ§åå·®ï¼Œä»è€Œå°†å•ä¸ªæ ·æœ¬çš„éšç§ä¿¡æ¯**æ¨¡ç³ŠåŒ–**ã€‚\n",
    "\n",
    "![å·®åˆ†éšç§ç¤ºæ„å›¾](./imgs/dp1.jpg)\n",
    "\n",
    "æ¯”å¦‚ï¼Œå¯¹äº data-1 å’Œ data-2 ä¸¤æ¬¡æŸ¥è¯¢ï¼ŒæœªåŠ å·®åˆ†éšç§ä¿æŠ¤ï¼Œåˆ™æ¨¡å‹ä¼šè¾“å‡º 2 å’Œ 3 ï¼Œæ˜æ˜¾å­˜åœ¨åŒºåˆ«ï¼Œåˆ™å¯ä»¥ä»è¿™ä¸ªåŒºåˆ«åˆ†æç‰¹å®šæ ·æœ¬å±æ€§ï¼Œè€ŒåŠ äº†å·®åˆ†éšç§æŠ€æœ¯ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¾“å‡º 2.5 å·¦å³çš„æ•°æ®ï¼Œä½¿å¾— $D$ å’Œ $D'$ ç›¸å·®çš„é‚£ä¸ªç‰¹å®šæ ·æœ¬æ¨¡ç³ŠåŒ–ã€‚\n",
    "\n",
    "ä»æ•°å­¦è§’åº¦å‡ºå‘ï¼Œå·®åˆ†éšç§ç¡®ä¿äº†ç›¸é‚»æ•°æ®é›† $D$ ä¸ $D'$ å¯¹ä»»æ„è¾“å‡ºç»“æœ $O$ çš„æ¦‚ç‡åˆ†å¸ƒæ»¡è¶³ $P[F(D) \\in O] \\le e^{\\epsilon} \\cdot P[F(D') \\in O]$ ï¼ˆFæ˜¯æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼‰ï¼Œä¹Ÿå°±æ˜¯ï¼š$D$ å’Œ $D'$ äº§ç”ŸåŒä¸€è¾“å‡º $O$ çš„**æ¦‚ç‡æ¯”å€¼**è¢«ä¸¥æ ¼é™åˆ¶åœ¨ $[e^{-\\epsilon}, e^{\\epsilon}]$ èŒƒå›´å†…ï¼Œ é€šè¿‡ä¸¥æ ¼çš„æ¦‚ç‡çº¦æŸâ€œæŠ¹é™¤â€å•ä¸ªæ ·æœ¬å¯¹æ¨¡å‹è¾“å‡ºçš„å†³å®šæ€§å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# å›ºå®šéšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿å›¾è¡¨ä¸­çš„ä¸­æ–‡èƒ½æ­£å¸¸æ˜¾ç¤º\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"Noto Sans CJK JP\", \"AR PL UMing JP\"]\n",
    "# plt.rcParams[\"font.family\"] = [\"Noto Sans CJK JP\"]\n",
    "plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "\n",
    "# ç¡®ä¿åœ¨Jupyter Notebookä¸­å¯ä»¥æ­£å¸¸æ˜¾ç¤ºå›¾è¡¨\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Differential_Privacy():\n",
    "    # å¯è§†åŒ–Îµå¯¹éšç§ä¿æŠ¤çš„å½±å“\n",
    "    epsilons = [0.1, 1, 5]\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for eps in epsilons:\n",
    "        plt.plot(x, np.exp(eps) * x, label=f'Îµ = {eps}')\n",
    "    \n",
    "    plt.plot(x, x, 'k--', label='æ— éšç§ä¿æŠ¤')\n",
    "    plt.xlabel('åŸå§‹æ¦‚ç‡ P[M(D\\'âˆˆS)]')\n",
    "    plt.ylabel('æ‰°åŠ¨åæ¦‚ç‡ä¸Šé™ e^Îµ * P[M(D\\'âˆˆS)]')\n",
    "    plt.title('ä¸åŒÎµå€¼å¯¹å·®åˆ†éšç§çº¦æŸçš„å½±å“')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e30994",
   "metadata": {},
   "outputs": [],
   "source": [
    "Differential_Privacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a7b4d",
   "metadata": {},
   "source": [
    "å›¾ä¸ºä¸åŒ $\\epsilon$ å€¼å¯¹åŸå§‹æ¦‚ç‡ $P$ çš„å½±å“ï¼Œ$\\epsilon$ å€¼è¶Šå°ï¼Œæ¦‚ç‡çº¦æŸè¶Šä¸¥æ ¼ï¼Œä¾‹å¦‚å›¾ä¸­ $\\epsilon=0.1$ æ—¶ï¼Œæ‰°åŠ¨åçš„æ¦‚ç‡æœ€å¤šæ˜¯åŸå§‹æ¦‚ç‡çš„ $e^{0.1} \\approx 1.105$ å€ã€‚\n",
    "\n",
    "ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼šæˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰åˆ¤æ–­æŸä¸ªæˆå‘˜æ˜¯å¦åœ¨æ•°æ®é›†ä¸­ï¼Œå¯¹äºæŸä¸ªæˆå‘˜ Aï¼Œåœ¨ä¸æ·»åŠ æ‰°åŠ¨æ—¶å¾ˆå®¹æ˜“é€šè¿‡ç›¸é‚»æ•°æ®é›†æ¥è¿›è¡Œåˆ¤æ–­ã€‚åŠ å…¥ $\\epsilon=0.1$ æ‰°åŠ¨ï¼Œå³ä½¿ A åœ¨æ•°æ®ä¸­ï¼Œç»“æœæ¦‚ç‡æœ€å¤šæ¯”æ²¡æœ‰ A æ—¶é«˜ $10.5\\%$ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475bb756",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## å·®åˆ†éšç§å®ç°\n",
    "å®ç°å·®åˆ†éšç§ï¼Œæœ€é‡è¦çš„å³æ˜¯æ·»åŠ å™ªå£°ï¼Œæ‹‰æ™®æ‹‰æ–¯æœºåˆ¶ï¼ˆLaplace Mechanismï¼‰å’Œé«˜æ–¯æœºåˆ¶ï¼ˆGaussian Mechanismï¼‰æ˜¯ä¸¤ç§åœ¨å·®åˆ†éšç§ä¸­å¸¸è§çš„**æ·»åŠ å™ªå£°**çš„æ–¹æ³•ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬å¯¹è¿™ä¸¤ç§æ–¹æ³•è¿›è¡Œé€ä¸€å­¦ä¹ ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cefabc",
   "metadata": {},
   "source": [
    "### æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e795e87",
   "metadata": {},
   "source": [
    "æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶é€šè¿‡å‘æ•°æ®æŸ¥è¯¢ç»“æœä¸­**æ·»åŠ ç¬¦åˆæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ($f(x|\\mu, b) = \\frac 1 {2b} e^{-\\frac{|x - \\mu|}{b}}$)çš„å™ªå£°**ï¼Œæ¥å®ç°å·®åˆ†éšç§ä¿æŠ¤ã€‚\n",
    "\n",
    "æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶æ·»åŠ å™ªå£°å°ï¼Œä¸”ä¸¥æ ¼æ»¡è¶³å·®åˆ†éšç§å®šä¹‰ï¼Œå› æ­¤åœ¨ç®€å•çš„å·®åˆ†éšç§ä¿æŠ¤åœºæ™¯ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶æ¥å®ç°å·®åˆ†éšç§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_mechanism(true_value, sensitivity, epsilon):\n",
    "    \"\"\"\n",
    "    å®ç°Laplaceæœºåˆ¶\n",
    "    \n",
    "    å‚æ•°:\n",
    "    true_value (float): çœŸå®æŸ¥è¯¢å€¼\n",
    "    sensitivity (float): æŸ¥è¯¢çš„æ•æ„Ÿåº¦\n",
    "    epsilon (float): éšç§é¢„ç®—\n",
    "    \n",
    "    è¿”å›:\n",
    "    float: æ·»åŠ å™ªå£°åçš„æŸ¥è¯¢ç»“æœ\n",
    "    \"\"\"\n",
    "\n",
    "    # TODOï¼šå®ç°æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶\n",
    "\n",
    "    # è®¡ç®—Laplaceåˆ†å¸ƒçš„å‚æ•°b = æ•æ„Ÿåº¦/Îµ\n",
    "    \n",
    "    # ä»Laplaceåˆ†å¸ƒä¸­é‡‡æ ·å™ªå£°\n",
    "    \n",
    "    # æ·»åŠ å™ªå£°åˆ°çœŸå®å€¼\n",
    "    \n",
    "    return noisy_value\n",
    "\n",
    "\n",
    "def visualize_laplace_mechanism():\n",
    "    # çœŸå®å€¼\n",
    "    true_value = 100\n",
    "    # æ•æ„Ÿåº¦\n",
    "    sensitivity = 1\n",
    "    # ä¸åŒçš„éšç§é¢„ç®—\n",
    "    epsilons = [0.1, 1, 10]\n",
    "    # ç”Ÿæˆå™ªå£°æ ·æœ¬\n",
    "    num_samples = 1000\n",
    "    samples = {}\n",
    "    \n",
    "    for eps in epsilons:\n",
    "        samples[eps] = [laplace_mechanism(true_value, sensitivity, eps) for _ in range(num_samples)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, eps in enumerate(epsilons):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        sns.histplot(samples[eps], kde=True)\n",
    "        plt.axvline(x=true_value, color='r', linestyle='--', label='çœŸå®å€¼')\n",
    "        plt.title(f'Îµ = {eps}')\n",
    "        plt.xlabel('æŸ¥è¯¢ç»“æœ')\n",
    "        plt.ylabel('é¢‘ç‡')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d012629",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥æŸ¥çœ‹ä¸€ä¸‹æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶çš„æ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_laplace_mechanism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d084cef",
   "metadata": {},
   "source": [
    "å®éªŒç»“æœç¬¦åˆå·®åˆ†éšç§çš„å®šä¹‰ï¼šéšç€éšç§é¢„ç®— $\\epsilon$ çš„å¢å¤§ï¼Œå™ªå£°å‡å°ï¼ŒæŸ¥è¯¢ç»“æœä¼šæ›´æ¥è¿‘çœŸå®å€¼ï¼Œä½†è¿™æ ·ä¹Ÿä¼šå¯¼è‡´éšç§ä¿æŠ¤ç¨‹åº¦é™ä½ï¼›åä¹‹ï¼Œéšç§é¢„ç®—å‡å°ä¼šå¢å¤§éšç§ä¿æŠ¤ç¨‹åº¦ï¼Œä½†ä¹Ÿä¼šå¯¹æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿè¾ƒå¤§çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7797b",
   "metadata": {},
   "source": [
    "### é«˜æ–¯æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555ad3a",
   "metadata": {},
   "source": [
    "ä¸æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶ç±»ä¼¼ï¼Œé«˜æ–¯æœºåˆ¶é€šè¿‡å‘æŸ¥è¯¢ç»“æœä¸­æ·»åŠ é«˜æ–¯å™ªå£°æ¥å®ç°å·®åˆ†éšç§ã€‚\n",
    "\n",
    "å¯¹äºå‡½æ•° $f(D)$ ï¼ˆDä¸ºæ•°æ®é›†ï¼‰ï¼Œé«˜æ–¯æœºåˆ¶ $M(D)$ å®šä¹‰ä¸ºï¼š$M(D) = f(D) + \\mathcal N(0, \\delta ^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524dd8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mechanism(true_value, sensitivity, epsilon, delta):\n",
    "    \"\"\"\n",
    "    å®ç°é«˜æ–¯æœºåˆ¶\n",
    "    \n",
    "    å‚æ•°:\n",
    "    true_value (float): çœŸå®æŸ¥è¯¢å€¼\n",
    "    sensitivity (float): æŸ¥è¯¢çš„æ•æ„Ÿåº¦\n",
    "    epsilon (float): éšç§é¢„ç®—\n",
    "    delta (float): å¤±è´¥æ¦‚ç‡\n",
    "    \n",
    "    è¿”å›:\n",
    "    float: æ·»åŠ å™ªå£°åçš„æŸ¥è¯¢ç»“æœ\n",
    "    \"\"\"\n",
    "\n",
    "    # TODOï¼šå®ç°é«˜æ–¯æœºåˆ¶\n",
    "\n",
    "    # è®¡ç®—é«˜æ–¯åˆ†å¸ƒçš„æ ‡å‡†å·®\n",
    "    \n",
    "    # ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·å™ªå£°\n",
    "    \n",
    "    # æ·»åŠ å™ªå£°åˆ°çœŸå®å€¼\n",
    "    \n",
    "    return noisy_value\n",
    "\n",
    "def visualize_gaussian_mechanism():\n",
    "    # çœŸå®å€¼\n",
    "    true_value = 100\n",
    "    # æ•æ„Ÿåº¦\n",
    "    sensitivity = 1\n",
    "    # éšç§å‚æ•°\n",
    "    epsilon = 1\n",
    "    deltas = [1e-3, 1e-5, 1e-7]\n",
    "    # ç”Ÿæˆå™ªå£°æ ·æœ¬\n",
    "    num_samples = 1000\n",
    "    samples = {}\n",
    "    \n",
    "    for delta in deltas:\n",
    "        samples[delta] = [gaussian_mechanism(true_value, sensitivity, epsilon, delta) for _ in range(num_samples)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, delta in enumerate(deltas):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        sns.histplot(samples[delta], kde=True)\n",
    "        plt.axvline(x=true_value, color='r', linestyle='--', label='çœŸå®å€¼')\n",
    "        plt.title(f'Î´ = {delta}')\n",
    "        plt.xlabel('æŸ¥è¯¢ç»“æœ')\n",
    "        plt.ylabel('é¢‘ç‡')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6dd129",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥æŸ¥çœ‹ä¸€ä¸‹é«˜æ–¯æœºåˆ¶çš„æ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52069d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gaussian_mechanism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358e307",
   "metadata": {},
   "source": [
    "é«˜æ–¯æœºåˆ¶é€šè¿‡ $\\delta$ å‚æ•°å®ç°äº†æ¾å¼›å·®åˆ†éšç§ $(\\epsilon, \\delta)-DP$ ï¼Œ $\\delta$ ç»™å·®åˆ†éšç§æä¾›äº†ä¸€ä¸ªå°æ¦‚ç‡éšç§æ³„éœ²çš„å¯èƒ½æ€§ï¼Œè¿™æ ·çš„å¥½å¤„æ˜¯æé«˜äº†æ•°æ®çš„å¯ç”¨æ€§ã€‚\n",
    "\n",
    "é«˜æ–¯æœºåˆ¶å¯¹é«˜ç»´æ•°æ®æˆ–éœ€è¦ç²¾ç¡®æ¢¯åº¦çš„åœºæ™¯æ›´åŠ å‹å¥½ï¼Œå› æ­¤æ›´é€‚åˆæ·±åº¦å­¦ä¹ ç­‰åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873dd74",
   "metadata": {},
   "source": [
    "## å·®åˆ†éšç§åº”ç”¨--äººå£æ•°æ®åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80808c58",
   "metadata": {},
   "source": [
    "è¿™ä¸€å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ç°å®åœºæ™¯ï¼š**äººå£æ•°æ®åˆ†æåœºæ™¯**ï¼Œæ¥å±•ç¤ºå·®åˆ†éšç§çš„å®é™…åº”ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_data_example():\n",
    "    \"\"\"äººå£æ•°æ®åˆ†æä¸­çš„åº”ç”¨ + å·®åˆ†éšç§ä¿æŠ¤\"\"\"\n",
    "    \n",
    "    # ç”Ÿæˆæ¨¡æ‹Ÿäººå£æ•°æ®\n",
    "    n = 1000  # æ ·æœ¬é‡\n",
    "    ages = np.random.normal(35, 10, n).astype(int)\n",
    "    incomes = np.random.lognormal(10, 0.5, n).astype(int)\n",
    "    education_levels = np.random.choice(['é«˜ä¸­', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«'], size=n, \n",
    "                                     p=[0.4, 0.4, 0.15, 0.05])\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'å¹´é¾„': ages,\n",
    "        'æ”¶å…¥': incomes,\n",
    "        'æ•™è‚²ç¨‹åº¦': education_levels\n",
    "    })\n",
    "    \n",
    "    # è®¡ç®—çœŸå®ç»Ÿè®¡é‡\n",
    "    true_mean_age = data['å¹´é¾„'].mean()\n",
    "    true_income_sum = data['æ”¶å…¥'].sum()\n",
    "    true_education_counts = data['æ•™è‚²ç¨‹åº¦'].value_counts()\n",
    "    \n",
    "    # æ•æ„Ÿåº¦è®¡ç®—\n",
    "    age_sensitivity = 1  # å‡å€¼æŸ¥è¯¢çš„æ•æ„Ÿåº¦ä¸º1ï¼ˆæ·»åŠ æˆ–åˆ é™¤ä¸€ä¸ªäººçš„å½±å“ï¼‰\n",
    "    income_sensitivity = data['æ”¶å…¥'].max()  # æ±‚å’ŒæŸ¥è¯¢çš„æ•æ„Ÿåº¦ä¸ºæœ€å¤§å€¼\n",
    "    education_sensitivity = 1  # è®¡æ•°æŸ¥è¯¢çš„æ•æ„Ÿåº¦ä¸º1\n",
    "    \n",
    "    # è®¾ç½®éšç§é¢„ç®—\n",
    "    epsilon_total = 1.0\n",
    "    \n",
    "    # åˆ†é…éšç§é¢„ç®—ï¼ˆç®€å•å¹³å‡åˆ†é…ï¼‰\n",
    "    epsilon_age = epsilon_total / 3\n",
    "    epsilon_income = epsilon_total / 3\n",
    "    epsilon_education = epsilon_total / 3\n",
    "    \n",
    "    # åº”ç”¨Laplaceæœºåˆ¶æ·»åŠ å™ªå£°\n",
    "    noisy_mean_age = laplace_mechanism(true_mean_age, age_sensitivity, epsilon_age)\n",
    "    noisy_income_sum = laplace_mechanism(true_income_sum, income_sensitivity, epsilon_income)\n",
    "    \n",
    "    # å¯¹åˆ†ç±»è®¡æ•°åº”ç”¨å·®åˆ†éšç§\n",
    "    noisy_education_counts = {}\n",
    "    for level, count in true_education_counts.items():\n",
    "        noisy_count = laplace_mechanism(count, education_sensitivity, epsilon_education)\n",
    "        noisy_education_counts[level] = max(0, noisy_count)  # ç¡®ä¿è®¡æ•°éè´Ÿ\n",
    "    \n",
    "    # ç»“æœå¯è§†åŒ–\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # å¹´é¾„å‡å€¼\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(['çœŸå®å€¼', 'å·®åˆ†éšç§å€¼'], [true_mean_age, noisy_mean_age])\n",
    "    plt.title('å¹³å‡å¹´é¾„')\n",
    "    plt.ylabel('å¹´é¾„')\n",
    "    \n",
    "    # æ”¶å…¥æ€»å’Œ\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(['çœŸå®å€¼', 'å·®åˆ†éšç§å€¼'], [true_income_sum, noisy_income_sum])\n",
    "    plt.title('æ€»æ”¶å…¥')\n",
    "    plt.ylabel('æ”¶å…¥')\n",
    "    \n",
    "    # æ•™è‚²ç¨‹åº¦åˆ†å¸ƒ\n",
    "    plt.subplot(1, 3, 3)\n",
    "    levels = list(true_education_counts.index)\n",
    "    true_counts = [true_education_counts[level] for level in levels]\n",
    "    noisy_counts = [noisy_education_counts[level] for level in levels]\n",
    "    \n",
    "    x = np.arange(len(levels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, true_counts, width, label='çœŸå®å€¼')\n",
    "    plt.bar(x + width/2, noisy_counts, width, label='å·®åˆ†éšç§å€¼')\n",
    "    plt.xticks(x, levels)\n",
    "    plt.title('æ•™è‚²ç¨‹åº¦åˆ†å¸ƒ')\n",
    "    plt.ylabel('äººæ•°')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(f\"çœŸå®å¹³å‡å¹´é¾„: {true_mean_age:.2f}, å·®åˆ†éšç§å¹³å‡å¹´é¾„: {noisy_mean_age:.2f}\")\n",
    "    print(f\"çœŸå®æ€»æ”¶å…¥: {true_income_sum:,}, å·®åˆ†éšç§æ€»æ”¶å…¥: {int(noisy_income_sum):,}\")\n",
    "    print(\"\\næ•™è‚²ç¨‹åº¦åˆ†å¸ƒ:\")\n",
    "    for level in levels:\n",
    "        print(f\"  {level}: çœŸå®å€¼={true_education_counts[level]}, å·®åˆ†éšç§å€¼={int(noisy_education_counts[level])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f92c93",
   "metadata": {},
   "source": [
    "ä½¿ç”¨å·®åˆ†éšç§åï¼Œæ•°æ®æ•´ä½“ç‰¹å¾åœ¨ä¿æŒä¸å˜çš„å‰æä¸‹ï¼Œå¯¹æ•°æ®è¿›è¡Œäº†ä¸€å®šç¨‹åº¦çš„æ‰°åŠ¨ï¼Œåœ¨æ•°æ®ä¿æŒåˆ†æä»·å€¼çš„æƒ…å†µä¸‹ï¼Œå¯¹äºå•ä¸ªæ ·æœ¬æ¥è¯´ï¼Œä¸æ˜¯é‚£ä¹ˆå®¹æ˜“ä»ç›¸é‚»æ•°æ®é›†ä¸­æ¨æ–­å‡ºæ¥äº†ï¼Œå®ç°äº†å¯¹å•ä¸ªæ ·æœ¬çš„éšç§ä¿æŠ¤ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c026b",
   "metadata": {},
   "source": [
    "## æ·±åº¦å­¦ä¹ ä¸­çš„å·®åˆ†éšç§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c62199",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥å°†å·®åˆ†éšç§ç†è®ºåº”ç”¨åˆ°æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¿æŠ¤æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œåº”ç”¨è¿‡ç¨‹ä¸­çš„ç”¨æˆ·éšç§ï¼Œå¹¶å°½å¯èƒ½ç»´æŠ¤æ¨¡å‹æ€§èƒ½ã€‚\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå·®åˆ†éšç§çš„å®ç°æ–¹å¼ä¸æ˜¯å¯¹æ•°æ®æ ·æœ¬ç›´æ¥æ·»åŠ å™ªå£°ï¼Œä¹Ÿä¸æ˜¯å¯¹æ¨¡å‹è¾“å‡ºæ·»åŠ å™ªå£°ï¼Œè€Œæ˜¯åœ¨**æ¨¡å‹è®­ç»ƒè¿‡ç¨‹**ä¸­ï¼Œå¯¹æ¨¡å‹çš„**æ¢¯åº¦ç©ºé—´**è¿›è¡Œ**æ·»åŠ å™ªå£°**ï¼Œ**é™åˆ¶å•ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦äº§ç”Ÿçš„å½±å“**ï¼Œé˜²æ­¢æ”»å‡»è€…é€šè¿‡æ¢¯åº¦çš„å˜åŒ–æ¨æ–­å‡ºå•ä¸ªæ ·æœ¬çš„å±æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå·®åˆ†éšç§è®­ç»ƒé€»è¾‘ä¸ºï¼š åŸå§‹æ•°æ® $\\xrightarrow{å‰å‘ä¼ æ’­}$ æ¨¡å‹è¾“å‡º $\\xrightarrow{æŸå¤±å‡½æ•°}$ æŸå¤±å€¼ $\\xrightarrow{åå‘ä¼ æ’­}$ åŸå§‹æ¢¯åº¦ $\\xrightarrow[æ·»åŠ å™ªå£°]{æ¢¯åº¦è£å‰ª}$ éšç§åˆè§„æ¢¯åº¦ $\\xrightarrow{ä¼˜åŒ–å™¨}$ æ¨¡å‹å‚æ•°æ›´æ–°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af6820",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä½¿ç”¨MNISTæ•°æ®é›†æ¥å®ç°æ·±åº¦å­¦ä¹ ä¸­çš„å·®åˆ†éšç§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafa483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(batch_size=64, val_ratio=0.2):\n",
    "    \"\"\"åŠ è½½MNISTæ•°æ®é›†å¹¶åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†\"\"\"\n",
    "    # æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ï¼‰\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNISTå‡å€¼/æ ‡å‡†å·®\n",
    "    ])\n",
    "    \n",
    "    # åŠ è½½è®­ç»ƒé›†\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    val_size = int(len(train_dataset) * val_ratio)\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # æµ‹è¯•é›†\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def plot_mnist_examples(dataset, num_examples=6):\n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(15, 2))\n",
    "    for i in range(num_examples):\n",
    "        img, label = dataset[i]\n",
    "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683605d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½MNISTæ•°æ®\n",
    "train_loader, val_loader, test_loader = load_mnist_data(batch_size=128)\n",
    "\n",
    "# æŸ¥çœ‹ç¤ºä¾‹å›¾åƒ\n",
    "plot_mnist_examples(datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd07a9",
   "metadata": {},
   "source": [
    "ç”±äºåœ¨å·®åˆ†éšç§æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¨¡å‹æ¢¯åº¦çš„æ•æ„Ÿæ€§ $Sensitivity = max_{D, D'} ||\\nabla L(D) - \\nabla L(D')||$ å®šä¹‰äº†ç›¸é‚»æ•°æ®é›†çš„æ¢¯åº¦çš„æœ€å¤§å¯èƒ½å·®å¼‚ã€‚\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦é™åˆ¶è¿™ä¸ªå·®å¼‚ï¼Œå¦åˆ™æ¢¯åº¦å¯èƒ½ä¼šå› ä¸ºä¸ªåˆ«æ ·æœ¬è€Œå‰§çƒˆæ³¢åŠ¨ï¼Œå¯¼è‡´éœ€è¦æ·»åŠ **å¤§é‡å™ªå£°**æ‰èƒ½æ»¡è¶³å·®åˆ†éšç§ï¼Œä½†æ˜¯è¿™æ ·åˆä¼šå› ä¸º**è¿‡å¤šå™ªå£°**å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb547082",
   "metadata": {},
   "source": [
    "ä¸€ä¸ªæœ‰æ•ˆçš„æ–¹æ³•æ˜¯åœ¨æ¢¯åº¦ç©ºé—´è¿›è¡ŒåŠ å™ªå‰ï¼Œå…ˆè®¾ç½®æ¢¯åº¦èŒƒæ•°çš„ä¸Šé™ï¼ˆå¦‚ $L_2$ èŒƒæ•°ä¸è¶…è¿‡ 1.0 ï¼ŒæŠŠæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦æ•æ„Ÿæ€§é™åˆ¶åœ¨å›ºå®šèŒƒå›´å†…ï¼š\n",
    "\n",
    "$$\n",
    "clipped\\_{grad} = grad \\cdot min(1, \\frac{clip\\_norm}{||grad||})\n",
    "$$\n",
    "\n",
    "è¿™æ ·ï¼Œæ— è®ºæŸå•ä¸ªæ ·æœ¬æ¢¯åº¦æœ‰å¤šå¤§ï¼Œå…¶å¯¹æ•´ä½“æ¢¯åº¦çš„è´¡çŒ®éƒ½ä¼šè¢«é™åˆ¶åœ¨ä¸€ä¸ªåˆç†åŒºé—´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¢¯åº¦ L2 èŒƒæ•°è£å‰ª\n",
    "def clip_gradient(grads, max_norm=1.0):\n",
    "    \"\"\"å¯¹æ¢¯åº¦è¿›è¡ŒL2èŒƒæ•°è£å‰ª\"\"\"\n",
    "\n",
    "    # TODOï¼šè¡¥å…¨æ¢¯åº¦è£å‰ªå‡½æ•°\n",
    "\n",
    "    clipped = []\n",
    "    for g in grads:\n",
    "        if g is not None:\n",
    "            # è®¡ç®—æ¢¯åº¦çš„L2èŒƒæ•°\n",
    "            # å¦‚æœèŒƒæ•°å¤§äº max_normï¼Œåˆ™è¿›è¡Œè£å‰ª\n",
    "            # è£å‰ªå…¬å¼: clipped_g = g * (max_norm / norm)\n",
    "        else:\n",
    "            # å¦åˆ™ä¸è¿›è¡Œè£å‰ª\n",
    "    return clipped\n",
    "\n",
    "\n",
    "# æ·±åº¦å­¦ä¹ ä¸‹çš„åŠ å™ªå‡½æ•°\n",
    "\n",
    "# é«˜æ–¯å™ªå£°\n",
    "def add_gaussian_noise(tensor, epsilon=1.0, delta=1e-5, sensitivity=1.0, **kargs):\n",
    "    if delta <= 0:\n",
    "        raise ValueError(\"Delta must be greater than 0\")\n",
    "    sigma = (math.sqrt(2 * math.log(1.25 / delta)) * sensitivity) / epsilon\n",
    "    grad_std = max(torch.std(tensor).item(), 1e-6)\n",
    "    sigma_adj = sigma * min(grad_std, 0.1)\n",
    "    return tensor + torch.normal(0, sigma_adj, size=tensor.shape, device=tensor.device)\n",
    "\n",
    "# æ‹‰æ™®æ‹‰æ–¯å™ªå£°\n",
    "def add_laplace_noise(tensor, epsilon=1.0, sensitivity=1.0, **kargs):\n",
    "    scale = sensitivity / epsilon\n",
    "    grad_std = max(torch.std(tensor).item(), 1e-6)\n",
    "    scale_adj = scale * min(grad_std, 0.1)\n",
    "    noise = np.random.laplace(0, scale_adj, size=tensor.shape)\n",
    "    return tensor + torch.tensor(noise, device=tensor.device, dtype=tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccda0b",
   "metadata": {},
   "source": [
    "ç„¶åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªCNNæ¨¡å‹ç”¨äºå·®åˆ†éšç§è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 7 * 7, 128), nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4069509",
   "metadata": {},
   "source": [
    "æ·±åº¦å­¦ä¹ ä¸­ï¼Œå·®åˆ†éšç§ä¸»è¦æ˜¯å¯¹æ¢¯åº¦ç©ºé—´è¿›è¡ŒåŠ å™ªï¼Œä½¿å¾—è§‚å¯Ÿåˆ°çš„æ¢¯åº¦å’Œä»»ä½•å•ä¸ªæ ·æœ¬çš„çœŸå®æ¢¯åº¦ $\\nabla _\\theta L$ ä¸å¯åŒºåˆ†ã€‚\n",
    "\n",
    "åœ¨æ¨¡å‹è®­ç»ƒå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®å·®åˆ†éšç§çš„å®šä¹‰ï¼Œåœ¨æ¨¡å‹**åŸå§‹æ¢¯åº¦**ä¸Šè¿›è¡Œè£å‰ªå’ŒåŠ å™ªï¼Œä½¿å¾—å…¶ç¬¦åˆéšç§åˆè§„çš„æ¢¯åº¦ï¼Œå†ä½¿ç”¨ä¼˜åŒ–å™¨æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c4f41",
   "metadata": {},
   "source": [
    "åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¹³å‡ä¸ºæ¯ä¸€ä¸ªepochçš„è®­ç»ƒåˆ†é…åˆé€‚çš„éšç§é¢„ç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc46386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_epoch(model, data_loader, loss_fn, optimizer, dp_config, max_grad=1.0):\n",
    "    \"\"\"æ‰§è¡Œä¸€ä¸ªepochçš„å·®åˆ†éšç§è®­ç»ƒ\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO: å®Œæˆå·®åˆ†éšç§è®­ç»ƒéƒ¨åˆ†\n",
    "\n",
    "        # 1. å¯¹åå‘ä¼ æ’­æ±‚å‡ºçš„æ¢¯åº¦è¿›è¡Œè£å‰ª\n",
    "\n",
    "        # 2. å¯¹æ¢¯åº¦ç©ºé—´æ·»åŠ å™ªå£°ï¼ˆå¯è‡ªè¡Œé€‰æ‹© laplace æˆ– gaussian å™ªå£°ï¼‰\n",
    "\n",
    "        # 3. ä½¿ç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°\n",
    "        \n",
    "        # ç»Ÿè®¡æŒ‡æ ‡\n",
    "        total_loss += loss.item()\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (pred == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(data_loader), (correct / total) * 100\n",
    "\n",
    "# å®Œæ•´è®­ç»ƒ\n",
    "def train_dp_model(train_loader, val_loader, dp_config, lr=0.01, max_grad=1.0, epochs=10):\n",
    "    \"\"\"å·®åˆ†éšç§è®­ç»ƒå‡½æ•°\"\"\"\n",
    "    model = build_cnn_model()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'privacy': []}\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒ | Îµ={dp_config['epsilon']}, æœºåˆ¶={dp_config['mechanism']}\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        train_loss, train_acc = run_training_epoch(\n",
    "            model, train_loader, loss_fn, optimizer, dp_config, max_grad\n",
    "        )\n",
    "        \n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, loss_fn)\n",
    "        \n",
    "        # è®°å½•éšç§é¢„ç®—\n",
    "        privacy_spent = epoch * (dp_config['epsilon'] / epochs)  # å¹³å‡åˆ†é…é¢„ç®—\n",
    "        \n",
    "        # æ‰“å°è¿›åº¦\n",
    "        print(f\"Epoch {epoch:2d}/{epochs:2d} | \"\n",
    "              f\"Train: Loss {train_loss:.4f} Acc {train_acc:.2f}% | \"\n",
    "              f\"Val:   Loss {val_loss:.4f} Acc {val_acc:.2f}% | \"\n",
    "              f\"éšç§æ¶ˆè€—: {privacy_spent:.4f}\")\n",
    "        \n",
    "        # ä¿å­˜å†å²è®°å½•\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['privacy'].append(privacy_spent)\n",
    "        \n",
    "        # é¢„ç®—è€—å°½ç»ˆæ­¢\n",
    "        if privacy_spent >= dp_config['epsilon']:\n",
    "            print(\"âš ï¸ éšç§é¢„ç®—è€—å°½ï¼Œåœæ­¢è®­ç»ƒï¼\")\n",
    "            break\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# è¯„ä¼°å‡½æ•°\n",
    "def evaluate_model(model, data_loader, loss_fn):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (pred == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(data_loader), (correct / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18039f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "def plot_training_history(history: dict):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒ/éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # æŸå¤±æ›²çº¿\n",
    "    axes[0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', c='blue')\n",
    "    axes[0].plot(history['val_loss'], label='éªŒè¯æŸå¤±', c='orange', linestyle='--')\n",
    "    axes[0].set_xlabel('Epoch'), axes[0].set_ylabel('Loss'), axes[0].grid(True), axes[0].legend()\n",
    "    \n",
    "    # å‡†ç¡®ç‡æ›²çº¿\n",
    "    axes[1].plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡', c='blue')\n",
    "    axes[1].plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', c='orange', linestyle='--')\n",
    "    axes[1].set_xlabel('Epoch'), axes[1].set_ylabel('Accuracy (%)'), axes[1].grid(True), axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout(), plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f63ee",
   "metadata": {},
   "source": [
    "ä»¥MNISTæ•°æ®é›†ä¸ºä¾‹ï¼Œå¯è§†åŒ–è®­ç»ƒä¸€ä¸ªä¸­ç­‰éšç§çš„CNNæ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2099b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®å·®åˆ†éšç§å‚æ•°\n",
    "dp_params = {\n",
    "    'epsilon': 1.0,       # éšç§é¢„ç®—\n",
    "    'delta': 1e-5,        # é«˜æ–¯æœºåˆ¶å¿…éœ€\n",
    "    'sensitivity': 1.0,   # æ¢¯åº¦æ•æ„Ÿåº¦\n",
    "    'mechanism': 'gaussian'  # å™ªå£°ç±»å‹\n",
    "}\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model, history = train_dp_model(train_loader, val_loader, dp_params, epochs=8)\n",
    "\n",
    "# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270e1b1",
   "metadata": {},
   "source": [
    "å¯ä»¥å‘ç°ï¼Œä½¿ç”¨å·®åˆ†éšç§è¿›è¡Œè®­ç»ƒåï¼ŒæŸå¤±åè€Œä¼šéšç€æ¯ä¸ªEpochçš„è®­ç»ƒè€Œå¢å¤§ï¼Œè¿™æ˜¯å·®åˆ†éšç§è®­ç»ƒå¯¼è‡´çš„æ­£å¸¸ç°è±¡ã€‚\n",
    "\n",
    "æ¢¯åº¦è£å‰ªã€å™ªå£°éƒ½ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿä¸€å®šå½±å“ã€‚æ‰€ä»¥åœ¨å®é™…è¿›è¡Œå·®åˆ†éšç§è®­ç»ƒæ—¶ï¼Œéœ€è¦åšå¥½æ€§èƒ½å’Œéšç§ä¿æŠ¤çš„å¹³è¡¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a69eb3",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹ä¸åŒéšç§é…ç½®è¿›è¡Œå·®åˆ†éšç§è®­ç»ƒï¼Œè§‚å¯Ÿæ¨¡å‹çš„è¡¨ç°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d06bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ä¸åŒéšç§çº§åˆ«çš„å®éªŒ\n",
    "def compare_privacy_effect():\n",
    "    \"\"\"æµ‹è¯•ä¸åŒepsilonå€¼å¯¹MNISTåˆ†ç±»æ€§èƒ½çš„å½±å“\"\"\"\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    train_loader, val_loader, test_loader = load_mnist_data(batch_size=128)\n",
    "    \n",
    "    # éšç§é…ç½®\n",
    "    privacy_settings = [\n",
    "        {'name': 'æ— éšç§', 'epsilon': float('inf'), 'delta': 0, 'mechanism': 'gaussian'},\n",
    "        {'name': 'å¼±éšç§', 'epsilon': 10.0, 'delta': 1e-5, 'mechanism': 'gaussian'},\n",
    "        {'name': 'ä¸­éšç§', 'epsilon': 1.0, 'delta': 1e-5, 'mechanism': 'gaussian'},\n",
    "        {'name': 'å¼ºéšç§', 'epsilon': 0.1, 'delta': 1e-5, 'mechanism': 'gaussian'},\n",
    "    ]\n",
    "    \n",
    "    # è¿è¡Œå®éªŒ\n",
    "    results = []\n",
    "    for cfg in privacy_settings:\n",
    "        print(f\"\\n=== è®­ç»ƒ {cfg['name']} (Îµ={cfg['epsilon']}) ===\")\n",
    "        if cfg['epsilon'] == float('inf'):\n",
    "            # æ— éšç§æƒ…å†µï¼šç›´æ¥è®­ç»ƒ\n",
    "            model = build_cnn_model()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            for _ in range(10):\n",
    "                run_training_epoch(model, train_loader, nn.CrossEntropyLoss(), optimizer,\n",
    "                                  {'epsilon':1.0, 'delta':1e-5, 'sensitivity':1.0, 'mechanism':'gaussian'},\n",
    "                                  max_grad=1.0)  # ä¸æ·»åŠ å™ªå£°\n",
    "            _, test_acc = evaluate_model(model, test_loader, nn.CrossEntropyLoss())\n",
    "            results.append({'name': cfg['name'], 'acc': test_acc, 'epsilon': cfg['epsilon']})\n",
    "        else:\n",
    "            # å·®åˆ†éšç§è®­ç»ƒ\n",
    "            dp_cfg = {\n",
    "                'epsilon': cfg['epsilon'],\n",
    "                'delta': cfg['delta'],\n",
    "                'sensitivity': 1.0,\n",
    "                'mechanism': cfg['mechanism']\n",
    "            }\n",
    "            model, _ = train_dp_model(train_loader, val_loader, dp_cfg, lr=0.001, epochs=8)\n",
    "            _, test_acc = evaluate_model(model, test_loader, nn.CrossEntropyLoss())\n",
    "            results.append({'name': cfg['name'], 'acc': test_acc, 'epsilon': cfg['epsilon']})\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\nğŸ“Š MNISTéšç§ä¿æŠ¤å¯¹æ¯”ï¼š\")\n",
    "    for res in results:\n",
    "        eps = \"âˆ\" if res['epsilon'] == float('inf') else f\"{res['epsilon']:.1f}\"\n",
    "        print(f\"{res['name']}: æµ‹è¯•å‡†ç¡®ç‡ {res['acc']:.2f}% (Îµ={eps})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50976ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ä¸åŒéšç§çº§åˆ«\n",
    "compare_privacy_effect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
