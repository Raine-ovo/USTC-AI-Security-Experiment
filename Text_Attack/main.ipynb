{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程前言\n",
    "\n",
    "此为 <<人工智能安全>> 课程第一部分: 文本对抗攻击实验部分."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此前我们探讨了连续型数据（如图像像素）的扰动方法，本节课将聚焦离散型数据---文本的对抗扰动技术。\n",
    "\n",
    "相较于图像在连续空间的可微特性，文本数据具有离散组合特征，因此其对抗攻击需要使用差异化的设计思路。\n",
    "\n",
    "文本对抗扰动指通过对原始文本进行局部语义保持性修改（如替换、插入或删除特定字符），生成人类可读但能导致NLP模型误判的对抗样本。\n",
    "\n",
    "文本扰动技术核心在于两点：\n",
    "1. 确保扰动后的文本符合语法规范且语义连贯\n",
    "2. 能通过最小化修改幅度使对抗样本逃逸模型检测并维持人类可读性\n",
    "\n",
    "以情感分类任务为例，将负面评价“这部电影非常糟糕”中关键形容词替换为语义弱化的“差强人意”，可能误导模型将其误判为中性情感。此类攻击揭示了NLP模型对语义细微变化的脆弱性。\n",
    "\n",
    "目前主流的文本对抗攻击方法可以按两个维度分类：\n",
    "1. 扰动粒度：根据修改单元分为字符级、词级或短语级攻击\n",
    "2. 生成策略：依据搜索算法分为基于梯度的优化方法或启发式替代策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 实验准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT是由Google提出的基于Transformer架构的预训练语言模型，通过双向上下文理解实现文本表征，bert-base-uncased-imdb 则是BERT在IMDB影评数据集上微调的版本，主要用于情感二分类（负面/正面评价），准确率约为 94%。\n",
    "\n",
    "本次实验，我们使用 bert-base-uncased-imdb 情感分类模型来作为文本对抗攻击的测试基准。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先对一个样本进行攻击，查看攻击效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 08:24:58.285204: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 08:24:58.315603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-13 08:24:58.899054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 定义原始文本\n",
    "text = \"The movie was fantastic! The acting was superb and the plot kept me engaged throughout.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于规则的扰动攻击**\n",
    "\n",
    "这种攻击通过人为设计的规则对文本进行修改，实现方法比较简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们来人为定义一个替换策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串替换规则\n",
    "perturbation_map = {\n",
    "    'a': ['@', 'ä', 'à', 'á'],\n",
    "    'e': ['3', 'é', 'è'],\n",
    "    'i': ['1', '!', 'í'],\n",
    "    'o': ['0', 'ö', 'ó'],\n",
    "    's': ['$', '5'],\n",
    "    't': ['7', '+']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法实现了字符级别的扰动。\n",
    "接下来，我们只需要将这个映射到样本中，即可生成对抗样本。\n",
    "\n",
    "为了保持对抗样本的隐蔽性，需要引入一个概率值，文本中满足映射的字符将以某种概率进行替换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符级扰动攻击\n",
    "def char_perturbation(texts, prob=0.2):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single_output = True\n",
    "    else:\n",
    "        single_output = False\n",
    "\n",
    "    perturbed_texts = []\n",
    "    for text in texts:\n",
    "        perturbed = []\n",
    "        for char in text.lower():\n",
    "            if char in perturbation_map and torch.rand(1).item() < prob:\n",
    "                choices = perturbation_map[char]\n",
    "                index = torch.multinomial(torch.ones(len(choices)), 1).item()\n",
    "                perturbed.append(choices[index])\n",
    "            else:\n",
    "                perturbed.append(char)\n",
    "        perturbed_texts.append(''.join(perturbed))\n",
    "\n",
    "    if single_output:\n",
    "        return perturbed_texts[0]\n",
    "    else:\n",
    "        return perturbed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看扰动效果和模型预测结果：\n",
    "\n",
    "> 由于样本太短，若效果不理想可多测试几次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The movie was fantastic! The acting was superb and the plot kept me engaged throughout.\n",
      "Perturbed Text: the movíé was fan7as+ic! +he ac7ing wa$ superb and the plot kept me engaged 7hroughout.\n",
      "\n",
      "Original Prediction (neg/pos): [0.00285911 0.9971409 ]\n",
      "Perturbed Prediction (neg/pos): [0.3807452  0.61925477]\n"
     ]
    }
   ],
   "source": [
    "# 测试原始样本和对抗样本的分类结果\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    return torch.softmax(outputs.logits, dim=1).detach().cpu().numpy()\n",
    "\n",
    "def output_adversarial_example_and_prediction(text, attack):\n",
    "    perturbed_text = attack(text)\n",
    "    print(\"Original Text:\", text)\n",
    "    print(\"Perturbed Text:\", perturbed_text)\n",
    "\n",
    "    original_prob = predict(text)\n",
    "    perturbed_prob = predict(perturbed_text)\n",
    "\n",
    "    print(\"\\nOriginal Prediction (neg/pos):\", original_prob[0])\n",
    "    print(\"Perturbed Prediction (neg/pos):\", perturbed_prob[0])\n",
    "\n",
    "output_adversarial_example_and_prediction(text, char_perturbation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于嵌入的扰动攻击**\n",
    "\n",
    "这种攻击方法利用词向量空间来生成对抗样本，即连续空间梯度与离散空间语义替换。\n",
    "\n",
    "模型首先通过 $embedding$ 层，将离散词汇映射至连续向量空间，进而依据攻击目标选择扰动策略：\n",
    "1. 连续梯度扰动（如 FGSM ）：基于梯度符号法直接在词向量空间施加微小扰动，在数学上能够保证 $\\epsilon-ball$ 的向量邻近性，但解码后可能落入无效词汇区域（见实验结果）。\n",
    "2. 离散语义替换（如 Word2Vec ）：在词嵌入空间中检索k-邻近有效词汇集合并选取一个进行替换，保证了对抗样本可读性。\n",
    "\n",
    "我们分别阐述这两种方法的实现细节："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_perturbation(model, input_text, labels, epsilon, tokenizer):\n",
    "    '''\n",
    "    使用 FGSM 对输入文本进行扰动\n",
    "    参数：\n",
    "        model: 目标模型\n",
    "        input_text: 输入文本\n",
    "        labels: 对应标签\n",
    "        epsilon: 扰动强度\n",
    "        tokenizer: 分词器\n",
    "    返回\n",
    "        perturbed_text: 扰动后的文本\n",
    "    '''\n",
    "\n",
    "    # 对输入文本进行分词并转换为张量\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 由于输入input的词向量不属于叶子节点，无法进行梯度计算，因此需要克隆一份作为叶子节点\n",
    "    # 计算时直接使用克隆后的 embeddings 的梯度作为词嵌入的梯度\n",
    "    embeddings = model.get_input_embeddings()(inputs['input_ids'])\n",
    "    embeddings = embeddings.detach().clone()\n",
    "    embeddings.requires_grad = True\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # 使用嵌入表示作为输入\n",
    "        outputs = model(inputs_embeds=embeddings, attention_mask=inputs['attention_mask'])\n",
    "        loss = nn.CrossEntropyLoss()(outputs.logits, labels.to(model.device))\n",
    "\n",
    "        gradients = torch.autograd.grad(loss, embeddings)[0]\n",
    "        sign_gradients = gradients.sign()\n",
    "\n",
    "        # 对嵌入表示进行扰动\n",
    "        perturbed_embeddings = embeddings + epsilon * sign_gradients\n",
    "\n",
    "    # 将扰动后的嵌入表示转换回输入 ID\n",
    "    perturbed_input_ids = torch.argmax(torch.matmul(perturbed_embeddings, model.get_input_embeddings().weight.t()), dim=-1)\n",
    "\n",
    "    perturbed_text = tokenizer.decode(perturbed_input_ids.squeeze(), skip_special_tokens=True)\n",
    "    return perturbed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The movie was fantastic! The acting was superb and the plot kept me engaged throughout.\n",
      "Perturbed Text: themori 780rada! 338 670 she superb 670 the plot kept 670 engaged halftime\n",
      "\n",
      "Original Prediction (neg/pos): [0.00285911 0.9971409 ]\n",
      "Perturbed Prediction (neg/pos): [0.5996884 0.4003116]\n"
     ]
    }
   ],
   "source": [
    "output_adversarial_example_and_prediction(text, lambda x: fgsm_perturbation(model, x, torch.tensor([1]), 0.1, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验表明，尽管FGSM攻击能够通过生成对抗样本误导模型分类，但其产生的文本可读性显著降低，违背了对抗扰动攻击需保持人类可读性的约束。\n",
    "\n",
    "这是因为，FGSM在词向量空间中搜索扰动时，虽能保证生成向量与原始词嵌入的几何邻近性（即语义相似性），但词嵌入空间到实际词汇的映射并非双射关系——经扰动后的向量可能落入\"空洞区域\"，无法对应词典中的有效词汇。这种词向量离散化过程中的语义断裂，最终导致对抗样本丧失语言连贯性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了保证语义的连贯性，我们可以使用基于预训练词向量模型（如Word2Vec）进行替换策略，该策略利用余弦相似度计算在词嵌入空间中检索目标词的Top-K语义邻近词，进而通过概率采样策略选取替代词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The movie was fantastic! The acting was superb and the plot kept me engaged throughout.\n",
      "Perturbed Text: that flick became fantastic ! another Gifted_heiress was superb and however Victorian_Londoner_interred keeping myself actively_engaged thoughout .\n",
      "\n",
      "Original Prediction (neg/pos): [0.00285911 0.9971409 ]\n",
      "Perturbed Prediction (neg/pos): [0.8104382  0.18956177]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"word2vec-google-news-300.model\"):\n",
    "    # 加载预训练的 Word2Vec 模型\n",
    "    vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    # 自选下载到本地与否\n",
    "    # vec_model.save(\"word2vec-google-news-300.model\")\n",
    "else:\n",
    "    from gensim.models import KeyedVectors\n",
    "    vec_model = KeyedVectors.load(\"word2vec-google-news-300.model\")\n",
    "\n",
    "def word_embedding_perturbation(vec_model, texts, num_words=10, prob=0.7):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single_output = True\n",
    "    else:\n",
    "        single_output = False\n",
    "\n",
    "    perturbed_texts = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        perturbed_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in vec_model and torch.rand(1).item() < prob:\n",
    "                similar_words = vec_model.most_similar(token, topn=num_words)\n",
    "                new_word = similar_words[torch.randint(0, num_words, (1,)).item()][0]\n",
    "                perturbed_tokens.append(new_word)\n",
    "            else:\n",
    "                perturbed_tokens.append(token)\n",
    "        perturbed_texts.append(' '.join(perturbed_tokens))\n",
    "\n",
    "    if single_output:\n",
    "        return perturbed_texts[0]\n",
    "    else:\n",
    "        return perturbed_texts\n",
    "\n",
    "output_adversarial_example_and_prediction(text, lambda x: word_embedding_perturbation(vec_model, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用 imdb 数据集中的测试集，来进行 bert-base-uncased-imdb 模型的对抗测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "required_columns = ['input_ids', 'attention_mask', 'label', 'text']\n",
    "test_dataset.set_format(type='torch', columns=required_columns)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, attack):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        original_texts = batch[\"text\"]\n",
    "        if attack:\n",
    "            perturbed_texts = attack(model, original_texts)\n",
    "        else:\n",
    "            perturbed_texts = original_texts\n",
    "        inputs = tokenizer(perturbed_texts, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # 将预测的概率值转换为类别标签\n",
    "            pred_labels = torch.argmax(preds, dim=1).cpu().tolist()\n",
    "            predictions.extend(pred_labels)\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型准确率为:  89.09%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'BertForSequenceClassification' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m clean_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predictions)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型准确率为: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_perturbation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m char_perturbation_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predictions)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型准确率为: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_perturbation_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, dataloader, attack)\u001b[0m\n\u001b[1;32m      7\u001b[0m original_texts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attack:\n\u001b[0;32m----> 9\u001b[0m     perturbed_texts \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     perturbed_texts \u001b[38;5;241m=\u001b[39m original_texts\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mchar_perturbation\u001b[0;34m(texts, prob)\u001b[0m\n\u001b[1;32m      7\u001b[0m     single_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m perturbed_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m     11\u001b[0m     perturbed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BertForSequenceClassification' object is not iterable"
     ]
    }
   ],
   "source": [
    "predictions, true_labels = evaluation(model, test_dataloader, None)\n",
    "clean_accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"模型准确率为: {clean_accuracy * 100: .2f}%\")\n",
    "\n",
    "predictions, true_labels = evaluation(model, test_dataloader, char_perturbation)\n",
    "char_perturbation_accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"模型准确率为: {char_perturbation_accuracy * 100: .2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于字符替换规则较少，可能会导致模型准确率较高，这里仅作参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embedding_perturbation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m char_perturbation_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predictions)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型准确率为: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_perturbation_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 9\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, dataloader, attack)\u001b[0m\n\u001b[1;32m      7\u001b[0m original_texts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attack:\n\u001b[0;32m----> 9\u001b[0m     perturbed_texts \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     perturbed_texts \u001b[38;5;241m=\u001b[39m original_texts\n",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m evaluation(model, test_dataloader, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mword_embedding_perturbation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m char_perturbation_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predictions)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型准确率为: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_perturbation_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[75], line 24\u001b[0m, in \u001b[0;36mword_embedding_perturbation\u001b[0;34m(vec_model, texts, num_words, prob)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m vec_model \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m prob:\n\u001b[0;32m---> 24\u001b[0m         similar_words \u001b[38;5;241m=\u001b[39m \u001b[43mvec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         new_word \u001b[38;5;241m=\u001b[39m similar_words[torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, num_words, (\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m         perturbed_tokens\u001b[38;5;241m.\u001b[39mappend(new_word)\n",
      "File \u001b[0;32m~/miniconda3/envs/TA/lib/python3.8/site-packages/gensim/models/keyedvectors.py:852\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topn:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dists\n\u001b[0;32m--> 852\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mmatutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# ignore (don't return) keys from the input\u001b[39;00m\n\u001b[1;32m    854\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    855\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_key[sim \u001b[38;5;241m+\u001b[39m clip_start], \u001b[38;5;28mfloat\u001b[39m(dists[sim]))\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim \u001b[38;5;129;01min\u001b[39;00m best \u001b[38;5;28;01mif\u001b[39;00m (sim \u001b[38;5;241m+\u001b[39m clip_start) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_keys\n\u001b[1;32m    857\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/TA/lib/python3.8/site-packages/gensim/matutils.py:81\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(x, topn, reverse)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(x)[:topn]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# np >= 1.8 has a fast partial argsort, use that!\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m most_extreme \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m)\u001b[49m[:topn]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m most_extreme\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margsort(x\u001b[38;5;241m.\u001b[39mtake(most_extreme)))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/TA/lib/python3.8/site-packages/numpy/core/fromnumeric.py:871\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    794\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    869\u001b[0m \n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TA/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions, true_labels = evaluation(model, test_dataloader, lambda x: word_embedding_perturbation(vec_model, x))\n",
    "char_perturbation_accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"模型准确率为: {char_perturbation_accuracy * 100: .2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextAttack\n",
    "\n",
    "TextAttack是一个用于自然语言处理对抗攻击的 Python 库，它提供了丰富的攻击方法和模型接口。我们可以利用它实现快速方便的对抗攻击测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/raine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型和分词器...\n",
      "正在加载攻击方法...\n",
      "正在下载数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path attack_log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始攻击...\n",
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "import textattack\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "print(\"正在加载模型和分词器...\")\n",
    "# 将模型和分词器包装到 HuggingFaceModelWrapper 中\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "print(\"正在加载攻击方法...\")\n",
    "# 选择攻击方法，这里使用 TextFooler\n",
    "attack = TextFoolerJin2019.build(model_wrapper)\n",
    "\n",
    "# 手动加载数据集并添加 tqdm 进度条\n",
    "print(\"正在下载数据集...\")\n",
    "ds = load_dataset(\"imdb\", split=\"test\")\n",
    "# 转换为 textattack 的数据集格式\n",
    "dataset = HuggingFaceDataset(ds)\n",
    "ds.save_to_disk(\"./imdb_test_dataset\")\n",
    "\n",
    "# 进行攻击\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=10,  # 攻击的样本数量\n",
    "    log_to_csv=\"attack_log.csv\",  # 保存攻击日志到 CSV 文件\n",
    "    checkpoint_interval=5,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    disable_stdout=True\n",
    ")\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "\n",
    "# 使用 tqdm 显示攻击进度\n",
    "print(\"开始攻击...\")\n",
    "results = []\n",
    "for result in tqdm(attacker.attack_dataset(), total=min(attack_args.num_examples, len(dataset))):\n",
    "    results.append(result)\n",
    "\n",
    "# 输出攻击结果\n",
    "for result in results:\n",
    "    print(result.__str__(color_method=\"ansi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TextAttack 库还提供了端到端的使用方法，具体可参考 `additional_reading.ipynb` 文件。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
