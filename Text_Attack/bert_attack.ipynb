{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50bbddf",
   "metadata": {},
   "source": [
    "# BERT-based Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f74e59",
   "metadata": {},
   "source": [
    "# 1. 实验简介\n",
    "本次实验简单实现了[基于BERT模型的文本对抗攻击](https://arxiv.org/abs/2004.01970)的 BAE 攻击算法，该算法的基本思想是：使用BERT模型对重要词汇进行替换，保证文本语义改变不大的同时尽可能降低正确标签的预测概率。\n",
    "\n",
    "算法的流程如下：\n",
    " <p align=\"center\">\n",
    " <img src=\"./src/BAE-R_algorithm.png\" style=\"zoom: 60%\" >\n",
    " </p>\n",
    "\n",
    "该 BAE 算法有 \"R\"(Replace) 和 \"I\"(insert) 两个版本，以及 \"RI\" 一起使用的版本。本实验关注于 \"BAE-R\" 算法，即替换不同位置的单词以达到对抗攻击的效果。 \"BAE-I\" 算法为在不同单词左右插入新单词以达到对抗攻击的效果，基本思想与 \"BAE-R\" 类似。\n",
    "\n",
    "BAE-R 算法的基本流程如下：\n",
    "1. 先对 $S$ 中的每个token $t_i$ 计算其重要性 $I_i$（重要性的计算方法在后文）。\n",
    "2. 按重要性 $I_i$ 从高到低遍历 $i$ ，将第 $i$ 个单词转为 \"[MASK]\"，使用 BERT 模型预测 [MASK] 位置最可能的前 $K$ 个 token\n",
    "3. 过滤掉前 $k$ 个 token 中，替换后语义与原文不同的（语义的计算方法在后文）。\n",
    "4. 如果有替换的 token $t$ 使得模型的预测发生改变，则**返回**使得模型预测改变的替换中，与 $S$ 相似度最高的 $t$ 。\n",
    "5. 否则将该位置 $i$ 换成使得模型在正确标签 $y$ 上预测概率降低最大的 $t$ ，使用替换后的句子 $S_{adv} $继续 2 中的循环。\n",
    "6. 若循环结束，说明未找到对抗样本，返回 None 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0d0ef",
   "metadata": {},
   "source": [
    "# 2. 所需模型与库的下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbbdc6",
   "metadata": {},
   "source": [
    "下载 sentence-transformers 库，用于提取输入句子中的语义信息。使用方法见 huggingface 网页 (https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15203162",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb9dc0",
   "metadata": {},
   "source": [
    "下载 bert-MLM 模型至本地目录 `./model/bert-base-uncased` 。该模型为 maskedLM ，用于预测 [MASK] 位置的可能的单词。详情见模型网页 (https://huggingface.co/google-bert/bert-base-uncased) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffe720",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_ENDPOINT=https://hf-mirror.com huggingface-cli download google-bert/bert-base-uncased config.json model.safetensors tokenizer.json tokenizer_config.json vocab.txt --local-dir ./model/bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae1fd1",
   "metadata": {},
   "source": [
    "下载语义嵌入模型至本地目录 `./model/distiluse-base-multilingual-cased-v1` 。该模型用于将句子文本嵌入至 768 维的向量中，论文中使用嵌入向量的**余弦相似度**来衡量两个句子的语义信息一致度。详情见模型网页 (https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_ENDPOINT=https://hf-mirror.com huggingface-cli download sentence-transformers/distiluse-base-multilingual-cased-v1 --exclude \"*.bin\" \"*.h5\" \"onnx/*\" \"openvino/*\" --local-dir ./model/distiluse-base-multilingual-cased-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa801570",
   "metadata": {},
   "source": [
    "下载 bert-uncased 微调后的情感分类模型，该模型用于分类输入文本的情感信息。模型对每个样本的输出为二维向量：第 0 维代表分类为 negative 的概率，第 1 维代表分类为 positive 的概率。详情见模型网页 (https://huggingface.co/textattack/bert-base-uncased-imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59639d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_ENDPOINT=https://hf-mirror.com huggingface-cli download textattack/bert-base-uncased-imdb --exclude \"flax_model.msgpack\" --local-dir ./model/bert-base-uncased-imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f92d4d",
   "metadata": {},
   "source": [
    "下载imdb数据集（这里只需要测试集，因为已经下载了预训练的情感分类模型。感兴趣的同学可以尝试自己训练一个模型）。imdb 数据集中的每个条目有 `text` 和 `label` 两个域。`text` 域代表待分类的文本，`label` 域代表文本标签。详情见数据集网页 (https://huggingface.co/datasets/stanfordnlp/imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e22fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_ENDPOINT=https://hf-mirror.com huggingface-cli download stanfordnlp/imdb plain_text/test-00000-of-00001.parquet --local-dir ./dataset/imdb --repo-type dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7c1bc",
   "metadata": {},
   "source": [
    "# 3. 模型的加载与对应函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9627758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 加载 bert_MLM 模型\n",
    "bert_MLM = AutoModelForMaskedLM.from_pretrained(\"./model/bert-base-uncased\")\n",
    "bert_MLM.eval()\n",
    "bert_MLM = bert_MLM.to(\"cuda\")\n",
    "bert_MLM_tokenizer = AutoTokenizer.from_pretrained(\"./model/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c05dec",
   "metadata": {},
   "source": [
    "使用 bert-MLM 预测mask位置的前k个可能单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca35c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_predictions(model, tokenizer, sentence, k=10):\n",
    "    \"\"\"\n",
    "    返回一个列表，其中的每个元素是 k 个最可能的单词的列表\n",
    "    sentence 为输入句子，必须包含 [MASK] 词。可以是成批次的\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    mask_position = inputs.input_ids == tokenizer.mask_token_id\n",
    "    batch_indices, token_indices = mask_position.nonzero(as_tuple=True)\n",
    "\n",
    "    # 获取 [MASK] 位置的 logits\n",
    "    masked_logits = logits[batch_indices, token_indices]\n",
    "\n",
    "    # 计算 softmax 得到概率\n",
    "    # 获取 top-k 的词和概率\n",
    "    predictions = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "    topk_preds, topk_ids = torch.topk(predictions, k, dim=-1)\n",
    "\n",
    "    # 解码 top-k 词\n",
    "    topk_tokens = [tokenizer.convert_ids_to_tokens(row.tolist()) for row in topk_ids]\n",
    "    \n",
    "    return topk_tokens\n",
    "\n",
    "# 测试输入\n",
    "test_sentence = [\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"The capital of China is [MASK].\",\n",
    "]\n",
    "print(top_k_predictions(bert_MLM, bert_MLM_tokenizer, test_sentence, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f01bb",
   "metadata": {},
   "source": [
    "加载 imdb 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = load_dataset(\"./dataset/imdb\", split=\"test\")\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "# 打印 dataset 相关的信息\n",
    "print(next(iter(dataloader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbe672",
   "metadata": {},
   "source": [
    "加载预训练情感分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的情感分类模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./model/bert-base-uncased-imdb\")\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/bert-base-uncased-imdb\")\n",
    "\n",
    "acc_num = 0\n",
    "# 成 batch 测试模型在 imdb 测试集上的准确率\n",
    "# for batch in tqdm(dataloader):\n",
    "#     inputs = batch[\"text\"]\n",
    "#     labels = batch[\"label\"]\n",
    "#     inputs = tokenizer(inputs, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "#     preds = torch.argmax(outputs.logits, dim=1).cpu().detach().numpy()\n",
    "#     acc_num += (preds == labels.numpy()).sum()\n",
    "\n",
    "# 逐条测试模型在 imdb 测试集上的准确率\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(len(dataset))):\n",
    "#         inputs = dataset[i][\"text\"]\n",
    "#         labels = dataset[i][\"label\"]\n",
    "#         inputs = tokenizer(inputs, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#         preds = torch.argmax(outputs.logits, dim=1).cpu().detach().numpy()\n",
    "#         acc_num += (preds == labels).sum()\n",
    "#     print(\"Accuracy: \", acc_num / len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75859806",
   "metadata": {},
   "source": [
    "加载 USE 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = SentenceTransformer(\"./model/distiluse-base-multilingual-cased-v1\")\n",
    "use_model.eval()\n",
    "print(use_model)\n",
    "\n",
    "# 计算两个句子嵌入后的预先相似度\n",
    "def cosine_similarity(use_model, sentence_1, sentence_2):\n",
    "    sentence_1_embedding = use_model.encode(sentence_1)\n",
    "    sentence_2_embedding = use_model.encode(sentence_2)\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "        torch.tensor(sentence_1_embedding).unsqueeze(0),\n",
    "        torch.tensor(sentence_2_embedding).unsqueeze(0),\n",
    "        dim=1\n",
    "    )\n",
    "    return cosine_sim.item()\n",
    "\n",
    "# 测试两个句子之间的余弦相似度\n",
    "sentence = \"The food was good.\"\n",
    "sentence_1 = \"The food was so good.\"\n",
    "sentence_2 = \"The food was so bad.\"\n",
    "sentence_3 = \"The food was awful.\"\n",
    "print(cosine_similarity(use_model, sentence, sentence_1))\n",
    "print(cosine_similarity(use_model, sentence, sentence_2))\n",
    "print(cosine_similarity(use_model, sentence, sentence_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79de92",
   "metadata": {},
   "source": [
    "定义计算每个词重要性的函数。重要性的计算方法如下，将该词从原句子中去除，计算 logits 层中正确类别概率的减少量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceccfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义停用词表（这里采取很简单的形式）\n",
    "stop_words = set([\n",
    "    \"the\", \"and\", \"a\", \"to\", \"is\", \"in\", \"on\", \"for\", \"with\", \"this\", \"that\", \n",
    "    \"it\", \"of\", \"an\", \"as\", \"by\", \"be\", \"are\", \"was\", \"were\", \"am\", \"have\", \"has\", \"(\", \")\"\n",
    "])\n",
    "def word_importance(model, tokenizer, sentence, label):\n",
    "    # sentence 是一个输入句子（不支持成批次处理）\n",
    "    # 返回按重要性排序的：单词列表，对应的分数，单词所在的位置\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        original_score = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze(0)[label].item()\n",
    "\n",
    "        input_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "        words = []\n",
    "        position = []\n",
    "        word_importance = []\n",
    "        for i in range(len(input_words)):\n",
    "            # 如果在停用词表中，则跳过\n",
    "            # 可以选择是否判断停用词\n",
    "            if input_words[i] in stop_words:\n",
    "                continue\n",
    "            exclude_input_words = input_words[:i] + input_words[i+1:]\n",
    "            exclude_input = tokenizer.convert_tokens_to_ids(exclude_input_words)\n",
    "            exclude_input = torch.tensor([exclude_input]).to(model.device)\n",
    "            attention_mask = torch.ones_like(exclude_input).to(model.device)\n",
    "\n",
    "            exclude_output = model(input_ids=exclude_input, attention_mask=attention_mask)\n",
    "            exclude_score = torch.nn.functional.softmax(exclude_output.logits, dim=-1).squeeze(0)[label].item()\n",
    "            position.append(i)\n",
    "            words.append(input_words[i])\n",
    "            word_importance.append(original_score - exclude_score)\n",
    "\n",
    "        word_importance = torch.tensor(word_importance)\n",
    "\n",
    "        sorted_indice = torch.argsort(word_importance, descending=True)\n",
    "        sorted_word = [words[i] for i in sorted_indice.tolist()]\n",
    "        sorted_importance = [word_importance[i].item() for i in sorted_indice.tolist()]\n",
    "        position = [position[i] for i in sorted_indice.tolist()]\n",
    "\n",
    "        return sorted_word, sorted_importance, position\n",
    "\n",
    "# 测试 word_importance 函数\n",
    "sentence = \"This film offers many delights and surprises.\"\n",
    "label = 0\n",
    "\n",
    "sorted_words, sorted_importance, position = word_importance(model, tokenizer, sentence, label)\n",
    "print(\"sentence: \" + dataset[0][\"text\"])\n",
    "\n",
    "cnt = 0\n",
    "for word, importance, pos in zip(sorted_words, sorted_importance, position):\n",
    "    print(\"|-| word: \\\"{}\\\", importance: {}, position: {}\".format(word, importance, pos))\n",
    "    if cnt > 10:\n",
    "        break\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace39ebd",
   "metadata": {},
   "source": [
    "进行 bert-attack 攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d53e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = '\\033[31m'\n",
    "GREEN = '\\033[32m'\n",
    "RESET = '\\033[0m'\n",
    "def bert_attack(model, tokenizer, use_model, replace_model, replace_tokenizer, sentence, label, k=10, threshold=0.8):\n",
    "\n",
    "    original_sentence = sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    original_score = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze(0)[label].item()\n",
    "    # 计算原来样本的预测值\n",
    "    preds = torch.argmax(outputs.logits, dim=1).item()  \n",
    "    # 将单词按重要性排序\n",
    "    sorted_words, _, position = word_importance(model, tokenizer, sentence, label)\n",
    "    while True:\n",
    "        for i in position:\n",
    "            input_words = tokenizer.tokenize(sentence)\n",
    "            # 替换单词\n",
    "            new_sentence = input_words[:i] + [\"[MASK]\"] + input_words[i+1:]\n",
    "            new_sentence = tokenizer.convert_tokens_to_string(new_sentence)\n",
    "            # 得到前 k 个可能的替换单词\n",
    "            candidate_list = top_k_predictions(replace_model, replace_tokenizer, new_sentence, k=k)[0]\n",
    "\n",
    "            similarity = []\n",
    "            for word in candidate_list:\n",
    "                new_sentence = input_words[:i] + [word] + input_words[i+1:]\n",
    "                new_sentence = tokenizer.convert_tokens_to_string(new_sentence)\n",
    "                # 计算替换单词后的余弦相似度\n",
    "                sim = cosine_similarity(use_model, original_sentence, new_sentence)\n",
    "                similarity.append(sim)\n",
    "            # 将候选单词和相似度按相似度排序\n",
    "            sorted_indice = torch.argsort(torch.tensor(similarity), descending=True)\n",
    "            candidate_list = [candidate_list[i] for i in sorted_indice.tolist()]\n",
    "            similarity = [similarity[i] for i in sorted_indice.tolist()]\n",
    "\n",
    "            reduction = []\n",
    "            for word, sim in zip(candidate_list, similarity):\n",
    "                # 若当前相似度小于阈值，则跳出循环\n",
    "                if sim < threshold:\n",
    "                    break\n",
    "                new_sentence = input_words[:i] + [word] + input_words[i+1:]\n",
    "                new_sentence = tokenizer.convert_tokens_to_string(new_sentence)\n",
    "\n",
    "                new_inputs = tokenizer(new_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "                new_outputs = model(**new_inputs)\n",
    "                new_score = torch.nn.functional.softmax(new_outputs.logits,dim=-1).squeeze(0)[label].item()\n",
    "                # 将预测值的改变量添加到列表中，以用于排序\n",
    "                reduction.append(original_score - new_score)\n",
    "                 \n",
    "                new_preds = torch.argmax(new_outputs.logits, dim=1).item()\n",
    "\n",
    "                if new_preds != label:\n",
    "                    # 若攻击成功，则返回新的句子\n",
    "                    print(f\"replace {GREEN}{input_words[i]}{RESET} with {RED}{word}{RESET}, similarity: {sim}, \\\n",
    "prediction of label: {GREEN}{original_score}{RESET}--->{RED}{new_score}{RESET}\")\n",
    "                    print(\"attack success!\")\n",
    "                    return new_sentence\n",
    "            \n",
    "            if len(reduction) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 否则将当前单词替换为 导致目标预测概率下降最大 的单词\n",
    "            max_reduction_index = torch.argmax(torch.tensor(reduction)).item()\n",
    "            max_reduction_word = candidate_list[max_reduction_index]\n",
    "            max_reduction = reduction[max_reduction_index]\n",
    "\n",
    "            if max_reduction == 0:\n",
    "                continue\n",
    "\n",
    "            sentence = input_words[:i] + [max_reduction_word] + input_words[i+1:]\n",
    "            print(f\"replace {GREEN}{input_words[i]}{RESET} with {RED}{max_reduction_word}{RESET}, similarity: {similarity[max_reduction_index]}\\\n",
    ", prediction of label: {GREEN}{original_score}{RESET}--->{RED}{original_score - max_reduction}{RESET}\")\n",
    "            \n",
    "            sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "            outputs = model(**inputs)\n",
    "            original_score = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze(0)[label].item()\n",
    "\n",
    "        print(\"Attack failed!\")\n",
    "        return None\n",
    "\n",
    "# 使用 dataset[0] 来演示攻击效果\n",
    "# 以下代码为展示输出的代码\n",
    "sentence = dataset[0][\"text\"]\n",
    "label = dataset[0][\"label\"]\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "outputs = model(**inputs)\n",
    "original_score = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze(0)\n",
    "preds = torch.argmax(outputs.logits, dim=1).item()\n",
    "label = preds\n",
    "\n",
    "with torch.no_grad():\n",
    "    attack_sentence = bert_attack(model, tokenizer, use_model, bert_MLM, bert_MLM_tokenizer, sentence, label, k=10, threshold=0.8)\n",
    "\n",
    "    if attack_sentence is not None:    \n",
    "        inputs = tokenizer(attack_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        attack_preds = torch.argmax(outputs.logits, dim=1).item()\n",
    "        attack_score = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze(0)\n",
    "\n",
    "        original_sentence = tokenizer.convert_tokens_to_string(tokenizer.tokenize(sentence))\n",
    "        # highlight the changed words\n",
    "        # 仅作简单的对比示例，未考虑分词不同的情况，因此颜色对比的结果可能不同\n",
    "        # cnt = 0\n",
    "        # original_tokenization = tokenizer.tokenize(original_sentence)\n",
    "        # attack_tokenization = tokenizer.tokenize(attack_sentence)\n",
    "        # for word in original_tokenization:\n",
    "        #     if attack_tokenization[cnt] != word:\n",
    "        #         original_tokenization = original_tokenization[:cnt] + [GREEN + word + RESET] + original_tokenization[cnt+1:]\n",
    "        #         attack_tokenization = attack_tokenization[:cnt] + [RED + attack_tokenization[cnt] + RESET] + attack_tokenization[cnt+1:]\n",
    "        #     cnt += 1\n",
    "\n",
    "        # original_sentence = tokenizer.convert_tokens_to_string(original_tokenization)\n",
    "        # attack_sentence = tokenizer.convert_tokens_to_string(attack_tokenization)\n",
    "\n",
    "        print(\"original sentence: \\n\" + original_sentence)\n",
    "        print(f\"original pred: {preds}\")\n",
    "        print(\"original score: Negative: {}, Positive: {}\".format(original_score[0].item(), original_score[1].item()))\n",
    "        print(\"attack sentence: \\n\" + attack_sentence)\n",
    "        print(f\"attack pred: {attack_preds}\")\n",
    "        print(\"attack score: Negative: {}, Positive: {}\".format(attack_score[0].item(), attack_score[1].item()))\n",
    "\n",
    "        print(\"cosine_similarity: \", cosine_similarity(use_model, original_sentence, attack_sentence))\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
