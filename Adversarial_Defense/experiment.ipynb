{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 课程前言\n",
    "此为《人工智能安全》实验课第二部分：对抗训练实验部分.\n",
    "\n",
    "**对抗攻击的本质**\n",
    "\n",
    "在上一章节中，我们学习了对抗攻击的核心原理：**对抗扰动攻击本质是一个有约束的优化问题**。攻击者试图在输入样本 $x$ 上添加微小扰动 $\\delta$，使其在扰动范数约束 $||\\delta|| \\le \\epsilon$ 下，最大化模型的损失函数：\n",
    "\n",
    "$$\n",
    "\\delta^* = \\arg \\max _{||\\delta|| \\le \\epsilon} \\mathcal{L} (h_\\theta(x + \\delta), y)\n",
    "$$\n",
    "\n",
    "这意味着，攻击者的目标是找到使模型 $h_\\theta$ 预测错误且扰动最小的对抗样本 $x + \\delta$。\n",
    "\n",
    "**对抗训练的目标**\n",
    "\n",
    "本节课我们将学习如何训练一个对对抗攻击具有**鲁棒性**的模型。其核心思想是通过优化模型参数 $\\theta$ ，使得即使输入被添加了最坏情况的扰动，模型仍能保持性能稳定。这一目标可形式化为一个**min-max**双层优化问题：\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg \\min _\\theta \\left[ \\frac{1}{|S|} \\sum_{x, y \\in S} \\max_{||\\delta|| \\le \\epsilon} \\mathcal{L}(h_\\theta(x + \\delta), y) \\right]\n",
    "$$\n",
    "\n",
    "+ 内层最大化（Max）：针对每个训练样本 $x$ ，寻找使损失最大的对抗扰动 $\\delta^*$，模拟攻击者的行为。\n",
    "+ 外层最小化（Min）：优化模型参数 $\\theta$，使得在最坏扰动下，所有训练样本的平均损失最小化。\n",
    "\n",
    "通过这种优化框架，模型在训练过程中不断暴露于“最困难”的对抗样本，从而学习到更强的鲁棒特征。与普通训练不同，对抗训练不仅要求模型在干净数据上表现良好，还需要在扰动数据上保持稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LightSource\n",
    "\n",
    "# CUDNN 是一个用于GPU加速库\n",
    "torch.backends.cudnn.deterministic = True #设置True以使其在加速时选择固定的操作，使实验具有复现性\n",
    "torch.backends.cudnn.benchmark = False #设置False以使其在加速时选择固定的操作，使实验具有复现性\n",
    "\n",
    "# 使用 MNIST 数据集进行测试\n",
    "mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "# 定义数据迭代器实例\n",
    "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)\n",
    "# 选择 device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 torch 种子，实现复现性\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)    \n",
    "\n",
    "# 卷积神经网络\n",
    "model_cnn = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),\n",
    "                          nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),\n",
    "                          Flatten(),\n",
    "                          nn.Linear(7*7*64, 100), nn.ReLU(),\n",
    "                          nn.Linear(100, 10)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为提升模型的鲁棒性，我们尝试采用上节课学习到的 **快速梯度符号法(FGSM)** 和 **投影梯度下降法(PGD)** 算法来计算微小扰动，进而开展对抗训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM算法，返回扰动 delta\n",
    "def fgsm(model, X, y, epsilon=0.1):\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "# PGD算法，返回扰动 delta\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=20, randomize=False):\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 对抗训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何进行对抗训练**\n",
    "\n",
    "我们的目标是优化如下对抗训练函数：\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg \\min _\\theta \\left[ \\frac{1}{|S|} \\sum_{x, y \\in S} \\max_{||\\delta|| \\le \\epsilon} \\mathcal{L}(h_\\theta(x + \\delta), y) \\right]\n",
    "$$\n",
    "\n",
    "这与普通训练函数\n",
    "\n",
    "$$\n",
    "\\theta = \\arg \\min _\\theta \\left[ \\frac{1}{|S|} \\sum_{x, y \\in S} \\mathcal{L}(h_\\theta(x), y) \\right]\n",
    "$$\n",
    "\n",
    "存在相似性，显然，我们能够在普通训练函数的基础上开展优化工作。\n",
    "\n",
    "与普通训练函数的优化思路一致，我们考虑采用梯度下降方法。若要通过随机梯度下降简单地优化 $\\theta$ ，就需要针对一些小批量数据，计算损失函数关于 $\\theta$ 的梯度，然后朝着该梯度的负方向更新参数。具体而言，我们要重复更新\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\frac {1}{|B|} \\sum_{x, y \\in B} \\nabla_\\theta \\max_{||\\delta||\\le \\epsilon} \\mathcal{L} (h_\\theta(x+\\delta), y)\n",
    "$$\n",
    "\n",
    "对于内层最大化函数的梯度计算，我们可以按以下两步进行：\n",
    "1）找到使损失函数取得最大值的扰动 $\\delta$。\n",
    "2）计算该最大值点处的正常梯度。相关梯度由以下给出：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\delta^*(x) = \\arg \\max \\mathcal{L}(h_\\theta(x+\\delta), y) \\notag \\\\\n",
    "\n",
    "&\\nabla_\\theta \\max_{||\\delta|| \\le \\epsilon} \\mathcal{L}(h_\\theta(x+\\delta), y) = \\nabla_\\theta \\mathcal{L}(h_\\theta(x+\\delta^*(x)), y) \\notag \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "我们可以使用如下伪代码来表示对抗训练的过程：\n",
    "\n",
    "对于每个 $epoch$ :\n",
    "\n",
    "1. 选择一个$batch$, 初始化梯度向量为 $0 $\n",
    "\n",
    "2. 对于样本 $(x, y) \\in batch$\n",
    "\n",
    "    +  使用对抗攻击算法求出最大扰动 $\\delta^*$\n",
    "\n",
    "        $\\delta^*(x) = \\arg \\max \\mathcal{L}(h_\\theta(x+\\delta), y)$\n",
    "    \n",
    "    + 计算 $x + \\delta^*$ 处的梯度\n",
    "\n",
    "        $g := g + \\nabla_\\theta \\mathcal{L}(h_\\theta(x+\\delta^*), y)$\n",
    "\n",
    "3. 更新模型参数\n",
    "\n",
    "    $\\theta := \\theta - \\frac {\\alpha}{|B|} g$\n",
    "\n",
    "实际上，后续的参数更新步骤可借助模型的反向传播机制直接完成，这里仅对对抗训练的基本原理进行阐述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回模型在干净数据集上一个 epoch 的训练/测试误差\n",
    "def epoch(loader, model, opt=None):\n",
    "    total_loss, total_err = 0.,0.\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        yp = model(X)\n",
    "        loss = nn.CrossEntropyLoss()(yp,y)\n",
    "        if opt: # optimizer: Training  None: Evaluation\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "    # 返回误差/损失\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# 返回模型在对抗数据集上一个 epoch 的训练/测试误差\n",
    "def epoch_adversarial(loader, model, attack, opt=None, **kwargs):\n",
    "    '''\n",
    "    对抗性训练函数\n",
    "\n",
    "    参数:\n",
    "        loader: 数据加载器\n",
    "        model: 模型\n",
    "        attack: 对抗攻击方法\n",
    "        opt: 优化器, None 表示评估, 不需要对模型进行训练\n",
    "        kwargs: 其他参数\n",
    "    返回:\n",
    "        total_err  / len(loader.dataset): 平均误差\n",
    "        total_loss / len(loader.dataset): 平均损失\n",
    "    '''\n",
    "\n",
    "\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们测试普通训练的模型的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model_cnn.parameters(), lr=1e-1)\n",
    "print(\"Train Error\", \"Test Error\", \"Adv Error\", sep=\"\\t\")\n",
    "\n",
    "# 模型在 MNIST 上进行训练\n",
    "for t in range(10):\n",
    "    train_err, train_loss = epoch(train_loader, model_cnn, opt)\n",
    "    test_err, test_loss = epoch(test_loader, model_cnn)\n",
    "    adv_err, adv_loss = epoch_adversarial(test_loader, model_cnn, pgd_linf)\n",
    "    # 调参使用\n",
    "    if t == 4:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-2\n",
    "    print(*(\"{:.6f}\".format(i) for i in (train_err, test_err, adv_err)), sep=\"\\t\")\n",
    "torch.save(model_cnn.state_dict(), \"model_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "干净模型测试集上，模型错误率为 $1\\%$，而在对抗样本测试集上，模型错误率为 $72\\%$。\n",
    "这与上一节的结论一致，普通模型对干净数据表现良好，但对微小对抗扰动极度脆弱，这表明模型依赖的**非鲁棒特征**易被攻击者利用。\n",
    "\n",
    "接下来，我们采用**PGD对抗训练**（$\\alpha=0.01, \\epsilon=0.1$）重新训练模型，并在相同数据集上评估："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义使用对抗性训练的模型\n",
    "model_cnn_robust = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "                                 nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),\n",
    "                                 nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "                                 nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),\n",
    "                                 Flatten(),\n",
    "                                 nn.Linear(7*7*64, 100), nn.ReLU(),\n",
    "                                 nn.Linear(100, 10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model_cnn_robust.parameters(), lr=1e-1)\n",
    "print(\"Train Error\", \"Test Error\", \"Adv Error\", sep=\"\\t\")\n",
    "for t in range(10):\n",
    "    train_err, train_loss = epoch_adversarial(train_loader, model_cnn_robust, pgd_linf, opt)\n",
    "    test_err, test_loss = epoch(test_loader, model_cnn_robust)\n",
    "    adv_err, adv_loss = epoch_adversarial(test_loader, model_cnn_robust, pgd_linf)\n",
    "    # 调参使用\n",
    "    if t == 4:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-2\n",
    "    print(*(\"{:.6f}\".format(i) for i in (train_err, test_err, adv_err)), sep=\"\\t\")\n",
    "torch.save(model_cnn_robust.state_dict(), \"model_cnn_robust.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行对抗训练后，模型的对抗样本错误率大幅下降，这说明模型学会了抵抗预设攻击强度的扰动。同时，模型在干净样本错误率几乎保持不变，这说明对抗训练没有破坏模型对原始任务的理解，其学到的是**本质鲁棒特征**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回模型在不同对抗攻击算法下的误差\n",
    "print(\"FGSM: \", epoch_adversarial(test_loader, model_cnn_robust, fgsm)[0])\n",
    "print(\"PGD, 40 iter: \", epoch_adversarial(test_loader, model_cnn_robust, pgd_linf, num_iter=40)[0])\n",
    "print(\"PGD, small_alpha: \", epoch_adversarial(test_loader, model_cnn_robust, pgd_linf, num_iter=40, alpha=0.05)[0])\n",
    "print(\"PGD, randomized: \", epoch_adversarial(test_loader, model_cnn_robust, pgd_linf, \n",
    "                                             num_iter=40, randomize=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 绘制损失函数曲面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以为什么对抗训练得到的模型拥有更好的鲁棒性呢？这个问题有很多种理论回答。但我们可以通过绘出普通模型与对抗训练模型的 loss 曲面图来观察一下它们之间的差异。以下代码将损失函数投影至输入空间的两个维度： x 轴代表随机方向， y 轴代表梯度方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in test_loader:\n",
    "    X,y = X.to(device), y.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss(model, X, epsilon):\n",
    "    # 使用 meshgrid 创建网格数据\n",
    "    Xi, Yi = np.meshgrid(np.linspace(-epsilon, epsilon,100), np.linspace(-epsilon,epsilon,100))\n",
    "    \n",
    "    # 该函数用于计算扰动为 delta 时的梯度方向，使用 sign 函数来获取每一维上的方向\n",
    "    def grad_at_delta(delta):\n",
    "        delta.requires_grad_(True)\n",
    "        nn.CrossEntropyLoss()(model(X+delta), y[0:1]).backward()\n",
    "        return delta.grad.detach().sign().view(-1).cpu().numpy()\n",
    "\n",
    "    # dir1 为 loss 函数的梯度方向\n",
    "    dir1 = grad_at_delta(torch.zeros_like(X, requires_grad=True))\n",
    "    delta2 = torch.zeros_like(X, requires_grad=True)\n",
    "    delta2.data = torch.tensor(dir1).view_as(X).to(device)\n",
    "    dir2 = grad_at_delta(delta2)\n",
    "    np.random.seed(0)\n",
    "    # dir2 为随机方向\n",
    "    dir2 = np.sign(np.random.randn(dir1.shape[0]))\n",
    "    \n",
    "    # 矩阵乘法，得到每个 (x,y) 对应的扰动下的输入。其中 x 轴坐标代表沿随机方向的扰动大小，y 轴坐标代表沿 loss 函数的梯度方向的扰动大小\n",
    "    # 矩阵乘法保证每个 Xi 中的每个元素与 dir2 （随机方向）中对应的元素相乘，每个 Yi 中的每个元素与 dir1 （loss 函数的梯度方向）中对应的元素相乘\n",
    "    all_deltas = torch.tensor((np.array([Xi.flatten(), Yi.flatten()]).T @ \n",
    "                              np.array([dir2, dir1])).astype(np.float32)).to(device)\n",
    "    yp = model(all_deltas.view(-1,1,28,28) + X)\n",
    "    # 得到不同扰动下的 loss 值，对应 (x,y) 处的 z 坐标\n",
    "    Zi = nn.CrossEntropyLoss(reduction=\"none\")(yp, y[0:1].repeat(yp.shape[0])).detach().cpu().numpy()\n",
    "    Zi = Zi.reshape(*Xi.shape)\n",
    "    #Zi = (Zi-Zi.min())/(Zi.max() - Zi.min())\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    # 使用 add_subplot 创建 3D 坐标轴\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ls = LightSource(azdeg=0, altdeg=200)\n",
    "    rgb = ls.shade(Zi, plt.cm.coolwarm)\n",
    "\n",
    "    surf = ax.plot_surface(Xi, Yi, Zi, rstride=1, cstride=1, linewidth=0,\n",
    "                       antialiased=True, facecolors=rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面绘制正常训练所得模型的 loss 曲面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(model_cnn, X[0:1], 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上文所述，该曲面中的每一点 $(x,y,z)$ 中的 $x$ 坐标的值代表沿随机方向的扰动大小， $y$ 坐标的值代表沿**梯度方向**的扰动大小， $z$ 坐标则代表损失函数在该 $(x,y)$ 扰动下的值。令 $\\pmb{\\lambda}$ 为随机方向的向量（各维度上的值均为 $1,0,-1$ ），代表其在该维度上的“方向”）， $\\pmb{\\mu}$ 为损失函数**梯度方向**的向量。则每一个点 $(x,y)$ 对应的 $z$ 值为\n",
    "$$\n",
    "z(x,y)=\\mathcal{L}(h_\\theta(\\pmb{X}+x\\pmb{\\lambda}+y\\pmb{\\mu}), y)\n",
    "$$\n",
    "其中 $\\pmb{X}$ 代表原始输入图像， $y$ 代表真实标签， $h_\\theta$ 代表模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从图像看出，正常训练所得模型的 loss 曲面沿 $y$ 轴方向十分“陡峭”，损失函数值变化非常剧烈，说明损失函数沿**梯度方向**下降/上升的非常快。这是正常模型得到的 loss 曲面。下面绘制对抗训练所得模型的 loss 曲面图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(model_cnn_robust, X[0:1], 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乍一看这个对抗训练所得模型的 loss 曲线似乎和上面的差不多。但仔细看看 $z$ 轴的值，会发现其相较于上张图小了很多。（如果放在同一比例尺下第二个曲面会变得非常**平坦**。）对抗训练所得模型的 loss 曲面非常**平坦**，无论是在随机方向上还是梯度方向上。而与之相对的，正常训练所得模型的 loss 值在梯度方向上变化非常大，在沿梯度方向“前进”了一段距离后的随机方向上 loss 值变化也较为明显。\n",
    "\n",
    "总的来说，对抗训练所得模型在实际应用中拥有较高的对抗鲁棒性，并且其拥有一个非常平坦的 loss 曲面。更多关于鲁棒性的原因机制还需未来的更进一步的研究来说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 评估不同扰动幅度下模型的鲁棒性\n",
    "为了实现对 `model_cnn_robust` 鲁棒性能的量化评估，下面将借助这套测试模型鲁棒性的代码，对该模型在测试数据集上的鲁棒性展开系统性测试 。\n",
    "\n",
    "下面来分析一下给定输入上下界时的对抗攻击：\n",
    "\n",
    "假设对于某一层的输入 $x$ 有上下界 $\\hat{l} \\leq x \\leq \\hat{u}$ ，则对于该层的输出有\n",
    "$$\n",
    "l\\leq Wx+b \\leq u\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "l=\\max\\{W,0\\}\\hat{l}+\\min\\{W,0\\}\\hat{u}+b \\\\\n",
    "u=\\min\\{W,0\\}\\hat{l}+\\max\\{W,0\\}\\hat{u}+b\n",
    "$$\n",
    "对于一个 $d$ 层的网络，对抗攻击的目标是**最小化**关于最后一层输出 $z_{d+1}$ 的“某种”线性函数 $c^Tz_{d+1}$ (可以是单位向量 $e_y$ ，其只有第 $y$ 位为1其余都为0)。假设倒数第二层有上下界 $\\hat{l}\\leq z_d\\leq \\hat{u}$ ，那么对抗攻击对应的优化问题转化为\n",
    "$$\n",
    "\\min c^Tz_{d+1} \\\\\n",
    "\\text{ s.t. } z_{d+1}=W_dz_d+b_d \\\\\n",
    "\\hat{l}\\leq z_d \\leq \\hat{u}\n",
    "$$\n",
    "该问题可以转化为以下等价形式\n",
    "$$\n",
    "\\min c^T(W_dz_d+b_d)=(c^TW_d)z_d+c^Tb_d \\\\\n",
    "\\text{ s.t. } \\hat{l}\\leq z_d \\leq\\hat{u}\n",
    "$$\n",
    "该优化问题的解为\n",
    "$$\n",
    "\\max\\{c^TW_d,0\\}\\hat{l}+\\min\\{c^TW_d,0\\}\\hat{u}+c^Tb_d\n",
    "$$\n",
    "下列代码中的 bound_propagation 函数用于计算在给定扰动 $\\epsilon$ 下，每一层的上下界。interval_based_bound 函数则用于计算上式。而当 $c=e_y-e_j, \\forall j\\neq y$ 时，若最终优化结果小于0，说明存在**可能的**对抗样本使得模型错误分类，可以据此分析模型的鲁棒特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bound_propagation 函数:\n",
    "    计算神经网络各层的输出区间边界, 遍历模型每一层, 对于不同的层类型(Flatten、nn.Linear、nn.Conv2d、nn.RELU)\n",
    "    使用对应公式计算其输出的上下界, 存储在 bounds 中\n",
    "'''\n",
    "def bound_propagation(model, initial_bound):\n",
    "    l, u = initial_bound\n",
    "    bounds = []\n",
    "    \n",
    "    for layer in model:\n",
    "        if isinstance(layer, Flatten):\n",
    "            l_ = Flatten()(l)\n",
    "            u_ = Flatten()(u)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            l_ = (layer.weight.clamp(min=0) @ l.t() + layer.weight.clamp(max=0) @ u.t() \n",
    "                  + layer.bias[:,None]).t()\n",
    "            u_ = (layer.weight.clamp(min=0) @ u.t() + layer.weight.clamp(max=0) @ l.t() \n",
    "                  + layer.bias[:,None]).t()\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            l_ = (nn.functional.conv2d(l, layer.weight.clamp(min=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  nn.functional.conv2d(u, layer.weight.clamp(max=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  layer.bias[None,:,None,None])\n",
    "            \n",
    "            u_ = (nn.functional.conv2d(u, layer.weight.clamp(min=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  nn.functional.conv2d(l, layer.weight.clamp(max=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) + \n",
    "                  layer.bias[None,:,None,None])\n",
    "            \n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            l_ = l.clamp(min=0)\n",
    "            u_ = u.clamp(min=0)\n",
    "            \n",
    "        bounds.append((l_, u_))\n",
    "        l,u = l_, u_\n",
    "    return bounds\n",
    "\n",
    "'''\n",
    "interval_based_bound 函数:\n",
    "    基于模型输出边界(bounds)计算神经网络模型的鲁棒性下界.\n",
    "'''\n",
    "def interval_based_bound(model, c, bounds, idx):\n",
    "    # 需要最后一层是线性层\n",
    "    cW = c.t() @ model[-1].weight\n",
    "    cb = c.t() @ model[-1].bias\n",
    "    \n",
    "    l,u = bounds[-2]\n",
    "    return (cW.clamp(min=0) @ l[idx].t() + cW.clamp(max=0) @ u[idx].t() + cb[:,None]).t()    \n",
    "\n",
    "'''\n",
    "evaluate_epoch_robust_bound 函数:\n",
    "    计算模型在一个 epoch 内的鲁棒性误差\n",
    "    返回测试集上模型的鲁棒性误差比例, 即, 统计在给定 epsilon 扰动范围内, 模型在各个类别上\n",
    "    不满足鲁棒性条件的样本数量占总样本数量的比例.\n",
    "'''\n",
    "def evaluate_epoch_robust_bound(loader, model, epsilon):\n",
    "    total_err = 0\n",
    "    \n",
    "    C = [-torch.eye(10).to(device) for _ in range(10)]\n",
    "    for y0 in range(10):\n",
    "        C[y0][y0,:] += 1\n",
    "    \n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        initial_bound = (X - epsilon, X + epsilon)\n",
    "        bounds = bound_propagation(model, initial_bound)\n",
    "        for y0 in range(10):\n",
    "            lower_bound = interval_based_bound(model, C[y0], bounds, y==y0)\n",
    "            total_err += (lower_bound.min(dim=1)[0] < 0).sum().item()\n",
    "    return total_err / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_epoch_robust_bound(test_loader, model_cnn_robust, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_epoch_robust_bound(test_loader, model_cnn_robust, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对上述代码的运行与分析可得，当扰动参数 $\\epsilon \\in [-0.1, 0.1]$ 时，模型 `model_cnn_robust` 无法展现出鲁棒特性（几乎所有测试集样本均不满足模型鲁棒性条件）；当扰动参数 $\\epsilon$ 收缩至 $[-0.0001, 0.0001]$时，模型的鲁棒性显著提升，测试集中约 $98\\%$ 的样本满足鲁棒性要求。\n",
    "\n",
    "换句话说，我们的模型在 $||\\epsilon||_\\infty \\le 0.0001$ 时具有良好的鲁棒性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 基于可证明的准则进行训练\n",
    "下面，我们将借助模型鲁棒性评估的结果，对模型训练过程实施优化。在训练或评估的每个批次中，先依据输入数据和指定的扰动范围，计算模型各层输出的区间边界。\n",
    "在此基础上，针对模型预测的类别，计算该类别下的鲁棒性下界，将这个下界累加到损失函数值 `loss` 中。\n",
    "\n",
    "这样，损失函数不仅包括了常规的分类错误，还融入了模型在面对输入扰动时的鲁棒性信息，模型根据这个损失函数更新参数，从而在模型分类准确率提升的同时，\n",
    "促使模型在指定扰动范围内的鲁棒性得到增强，应对输入数据可能存在的各种干扰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_robust_bound(loader, model, epsilon, opt=None):\n",
    "    '''\n",
    "    基于鲁棒性评估的对抗性训练函数\n",
    "\n",
    "    参数:\n",
    "        loader: 数据加载器\n",
    "        model: 模型\n",
    "        epsilon: 给定的扰动范围, 模型输入区间 [x-epsilon, x+epsilon]\n",
    "        opt: 优化器, None 表示评估, 不需要对模型进行训练\n",
    "    返回:\n",
    "        total_err  / len(loader.dataset): 平均误差\n",
    "        total_loss / len(loader.dataset): 平均损失\n",
    "\n",
    "    模型鲁棒性评估的算法可参考 evaluate_epoch_robust_bound 函数\n",
    "    '''\n",
    "    \n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们来借助这个带有鲁棒性评估信息的`loss`函数来训练我们的模型。训练出一个相对鲁棒的模型是比较讲究技巧的。如果我们直接按照扰动 $\\epsilon=0.1$ 来训练模型，模型会退化至预测每个数字为相同概率，而且无法在训练过程中恢复。训练需要从一个相对较小的 $\\epsilon$ 开始，然后在过程中逐步增大 $\\epsilon$ 直至达到我们需要的幅度。下面代码中的 $\\{\\epsilon\\}$ 序列的选取更偏启发性，如果精心设计可能效果会更好。但是这个 $\\{\\epsilon\\}$ 序列足以达到我们的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model_cnn_robust_2 = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1, stride=2), nn.ReLU(),\n",
    "                                   nn.Conv2d(32, 32, 3, padding=1, ), nn.ReLU(),\n",
    "                                   nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "                                   nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),\n",
    "                                   Flatten(),\n",
    "                                   nn.Linear(7*7*64, 100), nn.ReLU(),\n",
    "                                   nn.Linear(100, 10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model_cnn_robust_2.parameters(), lr=1e-1)\n",
    "eps_schedule = [0.0, 0.0001, 0.001, 0.01, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05] + 15*[0.1]\n",
    "\n",
    "print(\"Train Eps\", \"Train Loss*\", \"Test Err\", \"Test Robust Err\", sep=\"\\t\")\n",
    "for t in range(len(eps_schedule)):\n",
    "    train_err, train_loss = epoch_robust_bound(train_loader, model_cnn_robust_2, eps_schedule[t], opt)\n",
    "    test_err, test_loss = epoch(test_loader, model_cnn_robust_2)\n",
    "    adv_err, adv_loss = epoch_robust_bound(test_loader, model_cnn_robust_2, 0.1)\n",
    "\n",
    "    # 调参使用\n",
    "    if t == 4:\n",
    "       for param_group in opt.param_groups:\n",
    "           param_group[\"lr\"] = 1e-2\n",
    "\n",
    "    print(*(\"{:.6f}\".format(i) for i in (eps_schedule[t], train_loss, test_err, adv_err)), sep=\"\\t\")\n",
    "torch.save(model_cnn_robust_2.state_dict(), \"model_cnn_robust_2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后再来测试一下上述方法训练出来的模型的鲁棒性表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PGD, 40 iter: \", epoch_adversarial(test_loader, model_cnn_robust_2, pgd_linf, num_iter=40)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_epoch_robust_bound(test_loader, model_cnn_robust_2, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现训练出的模型具有较好的鲁棒性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
